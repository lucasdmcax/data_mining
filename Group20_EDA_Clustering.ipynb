{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642b95d6",
   "metadata": {},
   "source": [
    "# Customer Clustering: Multi-Perspective Approach\n",
    "\n",
    "This notebook identifies customer clusters from different perspectives (behavioral, profile), compares clustering algorithms, and merges the best solutions using hierarchical clustering on centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from umap import UMAP\n",
    "from minisom import MiniSom\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca93a",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "All custom functions for preprocessing, feature engineering, and aggregation are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_dataframe(df, columns, limits=(0.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Apply winsorization to each column in `columns`.\n",
    "    limits=(lower_pct, upper_pct) means: cap values at the 1st and 99th percentile.\n",
    "\n",
    "    Returns the winsorized copy of df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # winsorize returns masked arrays -> convert to normal array\n",
    "            df[col] = winsorize(df[col], limits=limits).data\n",
    "    return df\n",
    "\n",
    "def preprocess_flights(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the FlightsDB:\n",
    "    - Winsorize outliers\n",
    "    - Convert YearMonthDate to datetime\n",
    "    - Round down NumFlights and NumFlightsWithCompanions\n",
    "    - Set DistanceKM = 0 where NumFlights == 0\n",
    "    - Drop DollarCostPointsRedeemed\n",
    "    - Add log-transformed versions of skewed variables\n",
    "    - Create PointsUtilizationRatio = PointsRedeemed / PointsAccumulated\n",
    "    \"\"\"\n",
    "    df = flights_df.copy()\n",
    "\n",
    "    # 0. Winsorize outliers (Flights DB outliers are legitimate but skewed)\n",
    "    outlier_cols = [\n",
    "        'NumFlights', 'NumFlightsWithCompanions', 'DistanceKM', \n",
    "        'PointsAccumulated', 'PointsRedeemed'\n",
    "    ]\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 1. YearMonthDate -> datetime\n",
    "    if 'YearMonthDate' in df.columns:\n",
    "        df['YearMonthDate'] = pd.to_datetime(df['YearMonthDate'])\n",
    "\n",
    "    # 2. Round down flight counts and cast to int\n",
    "    for col in ['NumFlights', 'NumFlightsWithCompanions']:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.floor(df[col]).astype(int)\n",
    "\n",
    "    # 3. Fix logical inconsistency: DistanceKM must be 0 if NumFlights == 0\n",
    "    if {'NumFlights', 'DistanceKM'}.issubset(df.columns):\n",
    "        df.loc[df['NumFlights'] == 0, 'DistanceKM'] = 0\n",
    "\n",
    "    # 4. Drop perfectly correlated variable\n",
    "    if 'DollarCostPointsRedeemed' in df.columns:\n",
    "        df = df.drop(columns=['DollarCostPointsRedeemed'])\n",
    "\n",
    "    # 5. Log transforms for skewed numeric variables\n",
    "    log_cols = ['DistanceKM', 'PointsAccumulated', 'PointsRedeemed', 'NumFlights']\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # 6. Points utilisation ratio\n",
    "    if {'PointsRedeemed', 'PointsAccumulated'}.issubset(df.columns):\n",
    "        denom = df['PointsAccumulated'].replace({0: np.nan})\n",
    "        df['PointsUtilizationRatio'] = df['PointsRedeemed'] / denom\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_customers(customer_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the CustomerDB:\n",
    "    - Create cancellation flag from CancellationDate\n",
    "    - Group-median imputation (by LoyaltyStatus) for Income and Customer Lifetime Value\n",
    "    - Winsorize outliers (Income, CLV)\n",
    "    - Log transform Customer Lifetime Value and Income\n",
    "    - Encode Gender as binary\n",
    "    \"\"\"\n",
    "    df = customer_df.copy()\n",
    "\n",
    "    # 1. Cancellation flag\n",
    "    if 'CancellationDate' in df.columns:\n",
    "        df['CancelledFlag'] = df['CancellationDate'].notna().astype(int)\n",
    "\n",
    "    # 2. Group-median imputation by LoyaltyStatus\n",
    "    group_col = 'LoyaltyStatus'\n",
    "    cols_to_impute = ['Income', 'Customer Lifetime Value']\n",
    "    for col in cols_to_impute:\n",
    "        if col in df.columns and group_col in df.columns:\n",
    "            df[col] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "\n",
    "    # 3. Winsorize outliers\n",
    "    outlier_cols = ['Income', 'Customer Lifetime Value']\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 4. Log transforms\n",
    "    if 'Customer Lifetime Value' in df.columns:\n",
    "        df['CLV_log'] = np.log1p(df['Customer Lifetime Value'])\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'].clip(lower=0))\n",
    "\n",
    "    # 5. Gender encoding\n",
    "    if 'Gender' in df.columns:\n",
    "        df['Gender'] = df['Gender'].map({'female': 1, 'male': 0}).fillna(0).astype(int)\n",
    "\n",
    "    # 6. Education to Years (Ordinal Encoding)\n",
    "    if 'Education' in df.columns:\n",
    "        edu_map = {\n",
    "            'High School or Below': 12,\n",
    "            'College': 14,\n",
    "            'Bachelor': 16,\n",
    "            'Master': 18,\n",
    "            'Doctor': 21\n",
    "        }\n",
    "        df['Education'] = df['Education'].map(edu_map)\n",
    "        df['Education'] = df['Education'].fillna(16)\n",
    "\n",
    "    # 7. Turn marital status into a flag\n",
    "    if 'Marital Status' in df.columns:\n",
    "        df['Marital Status'] = np.where(df['Marital Status'] != 'Married', 1, 0)\n",
    "\n",
    "    # 8. Tenure\n",
    "    ref_date = pd.to_datetime('2022-01-01')\n",
    "    if 'EnrollmentDateOpening' in df.columns:\n",
    "        df['EnrollmentDateOpening'] = pd.to_datetime(df['EnrollmentDateOpening'])\n",
    "        df['TenureMonths'] = (ref_date - df['EnrollmentDateOpening']) / pd.Timedelta(days=30.44)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_customer_flight_features(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate monthly flight records into customer-level features:\n",
    "    - TotalFlights, TotalDistanceKM, TotalPointsAccumulated, TotalPointsRedeemed\n",
    "    - MeanPointsUtilization\n",
    "    - AverageFlightDistance\n",
    "    \"\"\"\n",
    "    id_col = 'Loyalty#'\n",
    "    df = flights_df.copy()\n",
    "    \n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(id_col)\n",
    "        .agg(\n",
    "            TotalFlights=('NumFlights', 'sum'),\n",
    "            TotalDistanceKM=('DistanceKM', 'sum'),\n",
    "            TotalPointsAccumulated=('PointsAccumulated', 'sum'),\n",
    "            TotalPointsRedeemed=('PointsRedeemed', 'sum'),\n",
    "            MeanPointsUtilization=('PointsUtilizationRatio', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Log transforms for aggregated features\n",
    "    for col in ['TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed']:\n",
    "        agg[f'{col}_log'] = np.log1p(agg[col])\n",
    "    \n",
    "    # Average flight distance\n",
    "    agg['AverageFlightDistance'] = agg['TotalDistanceKM'] / agg['TotalFlights'].replace({0: np.nan})\n",
    "\n",
    "    return agg\n",
    "\n",
    "def create_model_df(customer_df: pd.DataFrame, flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the final modeling dataframe:\n",
    "    1. Preprocess customers and flights\n",
    "    2. Build customer-level flight features\n",
    "    3. Merge datasets (Left Join)\n",
    "    4. Set Loyalty# as Index\n",
    "    5. Handle missing values\n",
    "    6. Encode categorical variables (OneHotEncoder)\n",
    "    7. Drop unnecessary columns\n",
    "    8. Scale numeric features (StandardScaler)\n",
    "    \"\"\"\n",
    "    # 1. Preprocess\n",
    "    cust_clean = preprocess_customers(customer_df)\n",
    "    flights_clean = preprocess_flights(flights_df)\n",
    "\n",
    "    # 2. Build flight features\n",
    "    flight_features = build_customer_flight_features(flights_clean)\n",
    "\n",
    "    # 3. Merge\n",
    "    model_df = cust_clean.merge(flight_features, on='Loyalty#', how='left')\n",
    "\n",
    "    # 4. Set Loyalty# as Index\n",
    "    if 'Loyalty#' in model_df.columns:\n",
    "        model_df.set_index('Loyalty#', inplace=True)\n",
    "\n",
    "    # 5. Handle Missing Values (Numeric)\n",
    "    numeric_cols_to_fill = model_df.select_dtypes(include=[np.number]).columns\n",
    "    model_df[numeric_cols_to_fill] = model_df[numeric_cols_to_fill].fillna(0)\n",
    "\n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'First Name', 'Last Name', 'CancellationDate', 'Customer Name',\n",
    "        'Country', 'Province or State', 'City', 'Postal Code',\n",
    "        'Latitude', 'Longitude', 'EnrollmentDateOpening', 'EnrollmentType',\n",
    "        'TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed',\n",
    "        'Customer Lifetime Value', 'Income'\n",
    "    ]\n",
    "    model_df = model_df.drop(columns=[c for c in cols_to_drop if c in model_df.columns], errors='ignore')\n",
    "\n",
    "    # 7. Separate Numeric and Categorical\n",
    "    categorical_cols = ['LoyaltyStatus', 'Location Code']\n",
    "    categorical_cols = [c for c in categorical_cols if c in model_df.columns]\n",
    "       \n",
    "    numeric_cols = model_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude binary/ordinal from scaling\n",
    "    unscaled_cols = []\n",
    "    for col in ['CancelledFlag', 'Marital Status', 'Gender']:\n",
    "        if col in numeric_cols:\n",
    "            numeric_cols.remove(col)\n",
    "            unscaled_cols.append(col)\n",
    "\n",
    "    # 8. OneHotEncoding\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first', dtype=int)\n",
    "    encoded_data = ohe.fit_transform(model_df[categorical_cols])\n",
    "    encoded_cols = ohe.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=encoded_cols, index=model_df.index)\n",
    "    \n",
    "    # 9. Scale Numeric Features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_numeric = scaler.fit_transform(model_df[numeric_cols])\n",
    "    df_numeric_scaled = pd.DataFrame(scaled_numeric, columns=numeric_cols, index=model_df.index)\n",
    "    \n",
    "    # 10. Combine\n",
    "    dfs_to_concat = [df_numeric_scaled, df_encoded]\n",
    "    if unscaled_cols:\n",
    "        dfs_to_concat.append(model_df[unscaled_cols])\n",
    "        \n",
    "    df_final = pd.concat(dfs_to_concat, axis=1)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def evaluate_clustering(algorithm_cls, X, param_grid, verbose = True, **kwargs):\n",
    "    algo_name = algorithm_cls.__name__\n",
    "    best_labels = None\n",
    "    best_metrics = (-np.inf, np.inf, -np.inf) \n",
    "    \n",
    "    # Store original index to return later\n",
    "    original_index = getattr(X, 'index', None)\n",
    "    \n",
    "    # Convert to numpy for stable math calculations\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    for values in product(*[param_grid[k] for k in keys]):\n",
    "        params = dict(zip(keys, values))\n",
    "        model = algorithm_cls(**params, **kwargs)\n",
    "        labels = model.fit_predict(X_np)\n",
    "        \n",
    "        n_clusters = len(np.unique(labels))\n",
    "        if n_clusters <= 1:\n",
    "            continue\n",
    "\n",
    "        # Calculate Metrics\n",
    "        sil = round(silhouette_score(X_np, labels), 2)\n",
    "        db = round(davies_bouldin_score(X_np, labels), 2)\n",
    "        \n",
    "        # R2 calculation (Variance Explained)\n",
    "        # Using X_np ensures total_var is a single float, not a Series\n",
    "        overall_mean = X_np.mean(axis=0)\n",
    "        total_var = np.sum((X_np - overall_mean) ** 2)\n",
    "        \n",
    "        between_var = 0\n",
    "        for k in np.unique(labels):\n",
    "            cluster_data = X_np[labels == k]\n",
    "            cluster_mean = cluster_data.mean(axis=0)\n",
    "            n_k = len(cluster_data)\n",
    "            between_var += n_k * np.sum((cluster_mean - overall_mean) ** 2)\n",
    "            \n",
    "        r2 = round(between_var / total_var, 2) if total_var > 0 else 0.0\n",
    "\n",
    "        # Tie-breaking logic\n",
    "        current_metrics = (sil, db, r2)\n",
    "        is_better = False\n",
    "        if sil > best_metrics[0]:\n",
    "            is_better = True\n",
    "        elif sil == best_metrics[0]:\n",
    "            if db < best_metrics[1]:\n",
    "                is_better = True\n",
    "            elif db == best_metrics[1]:\n",
    "                if r2 > best_metrics[2]:\n",
    "                    is_better = True\n",
    "\n",
    "        if is_better:\n",
    "            best_metrics = current_metrics\n",
    "            best_labels = labels\n",
    "\n",
    "        param_str = ', '.join([f'{k}={v}' for k, v in params.items()])\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{algo_name} ({param_str}): Clusters={n_clusters}, Sil={sil}, DB={db}, R2={r2}\")\n",
    "\n",
    "    # Return as Series to preserve the index\n",
    "    if original_index is not None:\n",
    "        return pd.Series(best_labels, index=original_index, name=f\"{algo_name}_labels\")\n",
    "    return best_labels\n",
    "\n",
    "def plot_k_distance(df, title, ax=None):\n",
    "    # Rule of thumb: k = 2 * number of dimensions\n",
    "    k = 2 * len(df.columns)\n",
    "    \n",
    "    # Fit Nearest Neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    distances, _ = neigh.kneighbors(df)\n",
    "    \n",
    "    # Sort distances to the k-th neighbor\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "    \n",
    "    # Plotting logic\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    ax.plot(k_distances, color='steelblue', linewidth=2)\n",
    "    ax.set_title(f\"K-Distance: {title} (k={k})\", fontsize=12)\n",
    "    ax.set_xlabel(\"Points sorted by distance\", fontsize=10)\n",
    "    ax.set_ylabel(f\"Distance to {k}-th neighbor\", fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "def apply_pca_2d(X, random_state=42):\n",
    "    \"\"\"Return a pandas DataFrame with a 2D PCA embedding for X.\"\"\"\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    embedding = pca.fit_transform(X_np)\n",
    "    idx = X.index if isinstance(X, pd.DataFrame) else None\n",
    "    cols = ['PCA1', 'PCA2']\n",
    "    return pd.DataFrame(embedding, columns=cols, index=idx)\n",
    "\n",
    "def apply_tsne_2d(X, random_state=42):\n",
    "    \"\"\"Return a pandas DataFrame with a 2D t-SNE embedding for X.\"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=random_state, n_jobs=-1)\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    embedding = tsne.fit_transform(X_np)\n",
    "    idx = X.index if isinstance(X, pd.DataFrame) else None\n",
    "    cols = ['TSNE1', 'TSNE2']\n",
    "    return pd.DataFrame(embedding, columns=cols, index=idx)\n",
    "\n",
    "# UMAP helper: compute 2D embedding and plotting function\n",
    "def apply_umap_2d(X, n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42):\n",
    "    \"\"\"Return a pandas DataFrame with a 2D UMAP embedding for X.\"\"\"\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric=metric, random_state=random_state)\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    embedding = umap_model.fit_transform(X_np)\n",
    "    idx = X.index if isinstance(X, pd.DataFrame) else None\n",
    "    cols = ['UMAP1', 'UMAP2']\n",
    "    return pd.DataFrame(embedding, columns=cols, index=idx)\n",
    "\n",
    "def plot_cluster(umap_df, pca_df, tsne_df, labels, main_title, palette='tab10', figsize=(24, 7), alpha=0.8, s=30):\n",
    "    \"\"\"\n",
    "    Plots UMAP, PCA, and t-SNE embeddings side-by-side in a single figure,\n",
    "    colored by cluster labels.\n",
    "\n",
    "    Args:\n",
    "        umap_df (pd.DataFrame): DataFrame with 2D UMAP embedding.\n",
    "        pca_df (pd.DataFrame): DataFrame with 2D PCA embedding.\n",
    "        tsne_df (pd.DataFrame): DataFrame with 2D t-SNE embedding.\n",
    "        labels (array-like): Labels for coloring the points (e.g., from a clustering algorithm).\n",
    "        main_title (str): The main title for the entire figure.\n",
    "        palette (str, optional): Color palette for the plot.\n",
    "        figsize (tuple, optional): Figure size for the entire figure.\n",
    "        alpha (float, optional): Opacity of the points.\n",
    "        s (int, optional): Size of the points.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    fig.suptitle(main_title, fontsize=16, y=0.98)\n",
    "\n",
    "    embeddings = {\n",
    "        'UMAP': umap_df,\n",
    "        'PCA': pca_df,\n",
    "        't-SNE': tsne_df\n",
    "    }\n",
    "\n",
    "    # Align labels once, assuming all embedding_dfs share the same index.\n",
    "    plot_labels = labels.reindex(umap_df.index) if isinstance(labels, pd.Series) else pd.Series(labels, index=umap_df.index)\n",
    "\n",
    "    for i, (name, df) in enumerate(embeddings.items()):\n",
    "        ax = axes[i]\n",
    "        x_col, y_col = df.columns\n",
    "        \n",
    "        sns.scatterplot(\n",
    "            x=x_col, \n",
    "            y=y_col, \n",
    "            data=df, \n",
    "            hue=plot_labels, \n",
    "            palette=palette, \n",
    "            s=s, \n",
    "            alpha=alpha, \n",
    "            legend='full' if i == 2 else False, # Show legend only on the last plot\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "    if axes[2].get_legend() is not None:\n",
    "        axes[2].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Helper: train a MiniSom and return codebook DataFrame + BMU series\n",
    "def som_codebook_for_evaluation(X, x=10, y=10, sigma=1.0, learning_rate=0.5, n_iter=2000, random_seed=42):\n",
    "    \"\"\"Train a MiniSom on DataFrame X and return (som, codebook_df, bmu_series).\"\"\"\n",
    "    X_np = X.values\n",
    "    som = MiniSom(x, y, X_np.shape[1], sigma=sigma, learning_rate=learning_rate, random_seed=random_seed)\n",
    "    som.random_weights_init(X_np)\n",
    "    som.train_random(X_np, n_iter)\n",
    "    # get weights (x, y, features) and reshape to (x*y, features)\n",
    "    weights = som.get_weights()\n",
    "    codebook = weights.reshape(x * y, -1)\n",
    "    feature_names = X.columns.tolist()\n",
    "    codebook_df = pd.DataFrame(codebook, columns=feature_names)\n",
    "    # Map each sample to BMU node id (0..x*y-1)\n",
    "    bmus = [som.winner(xi) for xi in X_np]\n",
    "    node_ids = [i * y + j for (i, j) in bmus]\n",
    "    bmu_series = pd.Series(node_ids, index=X.index, name='SOM_node')\n",
    "    return som, codebook_df, bmu_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e363a",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "\n",
    "Load customer and flight data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ceb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "customer_db = pd.read_csv(\"data/DM_AIAI_CustomerDB.csv\", index_col=0 )\n",
    "flights_db = pd.read_csv(\"data/DM_AIAI_FlightsDB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfe1cb",
   "metadata": {},
   "source": [
    "# Feature Set Definition\n",
    "\n",
    "Define behavioral and profile feature sets for clustering perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257255fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in customer database\n",
    "initial_rows = customer_db.shape[0]\n",
    "duplicated_loyalty_ids = customer_db[customer_db['Loyalty#'].duplicated()]['Loyalty#'].unique()\n",
    "customer_db = customer_db.drop_duplicates(subset=['Loyalty#'])\n",
    "dropped_rows = initial_rows - customer_db.shape[0]\n",
    "dropped_percentage = (dropped_rows / initial_rows) * 100\n",
    "\n",
    "print(f\"Dropped {dropped_rows} duplicate customers ({dropped_percentage:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the modeling dataset using the pipeline function\n",
    "# This handles preprocessing, merging, missing values, encoding, feature selection AND scaling\n",
    "model_df = create_model_df(customer_db, flights_db)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7666a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories for profile and behavior segmentation\n",
    "behavior_features = [\n",
    "    'TotalFlights_log', 'TotalDistanceKM_log', 'TotalPointsAccumulated_log', \n",
    "    'TotalPointsRedeemed_log', 'MeanPointsUtilization', 'AverageFlightDistance', \n",
    "]\n",
    "\n",
    "profile_features = [\n",
    "    'Income_log', 'Gender', 'Education', 'Marital Status', \n",
    "    'Location Code_Suburban', 'Location Code_Urban'\n",
    "\n",
    "]\n",
    "\n",
    "lifecycle_features = [\n",
    "    'LoyaltyStatus_Nova', 'LoyaltyStatus_Star', 'TenureMonths','CancelledFlag', 'CLV_log'\n",
    "]\n",
    "\n",
    "print(f\"Behavior features: {behavior_features}\")\n",
    "print(f\"Profile features: {profile_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the model dataframe\n",
    "model_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208e24c",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "Analyze and remove highly correlated features to improve clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4707b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Feature Selection\n",
    "# 1. Plot initial correlation matrix\n",
    "plt.figure(figsize=(24, 20))\n",
    "initial_corr = model_df.corr()\n",
    "sns.heatmap(initial_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Initial Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 2. Identify and remove highly correlated features (> 0.8)\n",
    "# We use the absolute correlation matrix\n",
    "corr_matrix = model_df.corr().abs()\n",
    "\n",
    "# We select the upper triangle of the correlation matrix. \n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# We drop the column (Feature B) if it has a correlation > 0.8 with any previous column (Feature A).\n",
    "# This way, Feature A is kept and Feature B is removed.\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "print(f\"Dropping highly correlated features: {to_drop}\")\n",
    "\n",
    "# Drop features from model_df\n",
    "model_df = model_df.drop(columns=to_drop)\n",
    "\n",
    "# Update feature lists to remove dropped columns\n",
    "behavior_features = [f for f in behavior_features if f not in to_drop]\n",
    "profile_features = [f for f in profile_features if f not in to_drop]\n",
    "\n",
    "print(f\"Updated behavior features: {behavior_features}\")\n",
    "print(f\"Updated profile features: {profile_features}\")\n",
    "\n",
    "# 3. Plot updated correlation matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(model_df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing Highly Correlated Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7e97d",
   "metadata": {},
   "source": [
    "# Outlier Detection\n",
    "\n",
    "Detect and remove multivariate outliers using DBSCAN before clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3419c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model_df as X_scaled (Loyalty# is now the index)\n",
    "dbscan = DBSCAN(eps=1.9, min_samples=20, n_jobs=1)\n",
    "dbscan_labels = dbscan.fit_predict(model_df)\n",
    "\n",
    "outlier_count = Counter(dbscan_labels)\n",
    "print(f\"DBSCAN results: {outlier_count}\")\n",
    "print(f\"Outliers detected: {outlier_count.get(-1, 0)}\")\n",
    "print(f\"Core customers: {outlier_count.get(0, 0)}\")\n",
    "\n",
    "core_mask = (dbscan_labels != -1)\n",
    "\n",
    "# Create clipped DataFrame without outliers\n",
    "model_df_clipped = model_df[core_mask]\n",
    "outliers_df = model_df[dbscan_labels == -1]\n",
    "\n",
    "print(f\"Core customers kept: {len(model_df_clipped):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eb7fd",
   "metadata": {},
   "source": [
    "# Clustering: Multiple Algorithms & Perspectives\n",
    "\n",
    "We now test several clustering algorithms, with different hyperparameters, This approach allows us to compare cluster quality and interpretability across perspectives and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature sets for clustering\n",
    "behavior_df = model_df_clipped[behavior_features]\n",
    "profile_df = model_df_clipped[profile_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction techniques to visualize clusters\n",
    "pca_be_df = apply_pca_2d(behavior_df)\n",
    "pca_pr_df = apply_pca_2d(profile_df)\n",
    "\n",
    "tsne_be_df = apply_tsne_2d(behavior_df)\n",
    "tsne_pr_df = apply_tsne_2d(profile_df)\n",
    "\n",
    "umap_be_df = apply_umap_2d(behavior_df)\n",
    "umap_pr_df = apply_umap_2d(profile_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fcca1",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on behavior features\n",
    "kmeans_param_grid = {'n_clusters': [2, 3, 4, 5, 6], 'init':['k-means++'], 'random_state':[42]}\n",
    "kmeans_behavior_labels = evaluate_clustering(KMeans, behavior_df, kmeans_param_grid)\n",
    "kmeans_behavior_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_profile_labels = evaluate_clustering(KMeans, profile_df, kmeans_param_grid)\n",
    "kmeans_profile_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_be_df, pca_be_df, tsne_be_df, kmeans_behavior_labels,\n",
    "    main_title=\"KMeans Clustering on Behavior Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25490cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_pr_df, pca_pr_df, tsne_pr_df, kmeans_profile_labels,\n",
    "    main_title=\"KMeans Clustering on Behavior Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba321d59",
   "metadata": {},
   "source": [
    "## Meanshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8553780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift on behavior features\n",
    "bandwidth = estimate_bandwidth(behavior_df, quantile=0.2, n_samples=500)\n",
    "meanshift_param_grid = {'bandwidth': [bandwidth], 'n_jobs': [-1]}\n",
    "meanshift_behavior_labels = evaluate_clustering(MeanShift, behavior_df, meanshift_param_grid)\n",
    "meanshift_behavior_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift on profile features\n",
    "bandwidth = estimate_bandwidth(profile_df, quantile=0.2, n_samples=500)\n",
    "meanshift_param_grid = {'bandwidth': [bandwidth], 'n_jobs': [-1]}\n",
    "meanshift_profile_labels = evaluate_clustering(MeanShift, profile_df, meanshift_param_grid)\n",
    "meanshift_profile_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_be_df, pca_be_df, tsne_be_df, meanshift_behavior_labels,\n",
    "    main_title=\"Meanshift Clustering on Behavior Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d837fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_pr_df, pca_pr_df, tsne_pr_df,meanshift_profile_labels,\n",
    "    main_title=\"Meanshift Clustering on Profile Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda644b8",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6979024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate figures\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Behavior DataFrame\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot_k_distance(behavior_df, \"Behavior Data\", ax=ax1)\n",
    "\n",
    "# Subplot 2: Profile DataFrame\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot_k_distance(profile_df, \"Profile Data\", ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on profile features\n",
    "dbscan_param_grid = {'eps': [0.2, 0.3, 0.4], 'min_samples': [10, 15, 20], 'n_jobs': [-1]}\n",
    "dbscan_behavior_labels = evaluate_clustering(DBSCAN, behavior_df, dbscan_param_grid)\n",
    "dbscan_behavior_labels.value_counts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90661d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on profile features\n",
    "dbscan_param_grid = {'eps': [0.1, 0.2, 0.3], 'min_samples': [200, 350, 500], 'n_jobs': [-1]}\n",
    "dbscan_profile_labels = evaluate_clustering(DBSCAN, profile_df, dbscan_param_grid)\n",
    "dbscan_profile_labels.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_be_df, pca_be_df, tsne_be_df, dbscan_behavior_labels,\n",
    "    main_title=\"DBSCAN Clustering on Behavior Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_pr_df, pca_pr_df, tsne_pr_df, dbscan_profile_labels,\n",
    "    main_title=\"DBSCAN Clustering on Profile Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f55db1",
   "metadata": {},
   "source": [
    "## Self Organizing Maps\n",
    "\n",
    "Simple workflow: train a SOM per perspective, cluster the SOM codebook with KMeans (use `evaluate_clustering`),\n",
    "and map neuron clusters back to samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM on behavior_df (using som_codebook_for_evaluation)\n",
    "som_behavior, codebook_beh, bmu_series_beh = som_codebook_for_evaluation(behavior_df)\n",
    "\n",
    "neuron_labels_beh = evaluate_clustering(KMeans, codebook_beh, verbose = False, param_grid=kmeans_param_grid)\n",
    "\n",
    "som_labels_beh = bmu_series_beh.map(neuron_labels_beh)\n",
    "\n",
    "# Calculate silhouette score for SOM clusters on behavior data\n",
    "som_silhouette_beh = silhouette_score(behavior_df, som_labels_beh)\n",
    "print(f\"SOM Clustering on Behavior Data: Silhouette Score = {som_silhouette_beh:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM on behavior_df (using som_codebook_for_evaluation)\n",
    "som_behavior, codebook_pro, bmu_series_pro = som_codebook_for_evaluation(profile_df)\n",
    "\n",
    "neuron_labels_pro = evaluate_clustering(KMeans, codebook_pro, verbose=False, param_grid=kmeans_param_grid)\n",
    "\n",
    "som_labels_pro = bmu_series_pro.map(neuron_labels_pro)\n",
    "\n",
    "# Calculate silhouette score for SOM clusters on profile data\n",
    "som_silhouette_pro = silhouette_score(profile_df, som_labels_pro)\n",
    "print(f\"SOM Clustering on Profile Data: Silhouette Score = {som_silhouette_pro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_be_df, pca_be_df, tsne_be_df, som_labels_beh,\n",
    "    main_title=\"SOM Clustering on Behavior Features\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(\n",
    "    umap_pr_df, pca_pr_df, tsne_pr_df, som_labels_pro,\n",
    "    main_title=\"SOM Clustering on Profile Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce23dc",
   "metadata": {},
   "source": [
    "## Fuzzy Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import skfuzzy as fuzz\n",
    "except ImportError:\n",
    "    print(\"scikit-fuzzy not installed. Skipping Fuzzy C-Means clustering.\")\n",
    "    fuzz = None\n",
    "\n",
    "if fuzz:\n",
    "    print(\"--- Fuzzy C-Means in Behavior Features ---\")\n",
    "    data_behavior = behavior_df.T.values\n",
    "    n_clusters_range = range(2, 7)\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n",
    "            data_behavior, n_clusters, 2, error=0.005, maxiter=1000, init=None, seed=42\n",
    "        )\n",
    "        \n",
    "        hard_labels = np.argmax(u, axis=0)\n",
    "        silhouette  = silhouette_score(behavior_df, hard_labels)\n",
    "        \n",
    "        print(f\"Clusters: {n_clusters}, Silhuette: {silhouette:.3f}, FPC: {fpc:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"--- Fuzzy C-Means in Profile Features ---\")\n",
    "\n",
    "    data_profile = profile_df.T.values\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n",
    "            data_profile, n_clusters, 2, error=0.005, maxiter=1000, init=None, seed=42\n",
    "        )\n",
    "        \n",
    "        hard_labels = np.argmax(u, axis=0)\n",
    "        silhouette = silhouette_score(profile_df, hard_labels)\n",
    "        \n",
    "        print(f\"Clusters: {n_clusters}, Silhueta: {silhouette:.3f}, FPC: {fpc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49caaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fuzz:\n",
    "    # --- Fuzzy C-Means Visualization for b. Features ---\n",
    "    \n",
    "    n_clusters_beh = 3 # Choosed based in silhouette score\n",
    "    \n",
    "    _, u_beh, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        behavior_df.T.values, n_clusters_beh, 2, error=0.005, maxiter=1000, seed=42\n",
    "    )\n",
    "    \n",
    "    hard_labels_beh = np.argmax(u_beh, axis=0)\n",
    "    fuzzy_behavior_labels = pd.Series(hard_labels_beh, index=behavior_df.index, name=\"fuzzy_behavior_labels\")\n",
    "\n",
    "    plot_cluster(\n",
    "        umap_be_df, pca_be_df, tsne_be_df,\n",
    "        fuzzy_behavior_labels,\n",
    "        main_title=\"Fuzzy C-Means Clustering on Behavior Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b56d7",
   "metadata": {},
   "source": [
    "##### Fuzzy Clustering Conclusions (Behavior)\n",
    "\n",
    "- **Optimal Clusters**: Based on the silhouette scores, **3 clusters** provide the best separation for the behavioral data.\n",
    "- **Cluster Quality**: The Fuzzy Partition Coefficient (FPC) is moderate, indicating some overlap between clusters, which is expected with fuzzy clustering. The visualization confirms that while the clusters are centered in distinct regions, their boundaries are not sharply defined, reflecting the \"fuzzy\" nature of the assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fuzz:\n",
    "    # --- Fuzzy C-Means Visualization for p. Features ---\n",
    "    \n",
    "    n_clusters_pro = 2 # Choosed based in silhouette score\n",
    "    \n",
    "    _, u_pro, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        profile_df.T.values, n_clusters_pro, 2, error=0.005, maxiter=1000, seed=42\n",
    "    )\n",
    "    \n",
    "    hard_labels_pro = np.argmax(u_pro, axis=0)\n",
    "    fuzzy_profile_labels = pd.Series(hard_labels_pro, index=profile_df.index, name=\"fuzzy_profile_labels\")\n",
    "\n",
    "    plot_cluster(\n",
    "        umap_pr_df, pca_pr_df, tsne_pr_df,\n",
    "        fuzzy_profile_labels,\n",
    "        main_title=\"Fuzzy C-Means Clustering on Profile Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca739f",
   "metadata": {},
   "source": [
    "##### Fuzzy Clustering Conclusions (Profile)\n",
    "\n",
    "- **Optimal Clusters**: The silhouette score is highest for **2 clusters** (0.494), suggesting a strong primary division within the profile data.\n",
    "- **Cluster Quality**: The FPC of 0.766 for 2 clusters is relatively high, indicating that the two segments are fairly distinct, although some fuzziness remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842225c3",
   "metadata": {},
   "source": [
    "# Merge Cluster Solutions\n",
    "\n",
    "After identifying the best clustering solutions for each perspective, merge them using hierarchical clustering on the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e97bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clustering results into final dataframe for analysis\n",
    "combined_features = list(behavior_df.columns) + list(profile_df.columns)\n",
    "\n",
    "# Create final dataframe with cluster labels\n",
    "final_df = model_df_clipped[combined_features].copy()\n",
    "final_df[\"behavior_cluster\"] = meanshift_behavior_labels.reindex(final_df.index).astype(int)\n",
    "final_df[\"profile_cluster\"]  = meanshift_profile_labels.reindex(final_df.index).astype(int)\n",
    "\n",
    "# Calculate centroids for each (behavior_cluster, profile_cluster) pair\n",
    "df_centroids = final_df.groupby([\"behavior_cluster\", \"profile_cluster\"])[combined_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9424867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram of Centroids using Hierarchical Clustering\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "X_centroids = df_centroids[combined_features].values  # matriz (n_centroids x n_features)\n",
    "\n",
    "# Hierarchical Clustering with Ward linkage\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=\"ward\",\n",
    "    metric=\"euclidean\",\n",
    "    distance_threshold=0,\n",
    "    n_clusters=None\n",
    ")\n",
    "\n",
    "hclust_labels = hclust.fit_predict(X_centroids)\n",
    "\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            current_count += 1  # leaf\n",
    "        else:\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "linkage_matrix = np.column_stack([\n",
    "    hclust.children_,\n",
    "    hclust.distances_,\n",
    "    counts\n",
    "]).astype(float)\n",
    "\n",
    "y_threshold = 2.7\n",
    "\n",
    "labels = [\n",
    "    f\"{idx[0]}-{idx[1]} (n={int(df_centroids.loc[idx, 'segment_size'])})\"\n",
    "    if \"segment_size\" in df_centroids.columns\n",
    "    else f\"{idx[0]}-{idx[1]}\"\n",
    "    for idx in df_centroids.index\n",
    "]\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=labels,\n",
    "    color_threshold=y_threshold,\n",
    "    above_threshold_color=\"k\"\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    y=y_threshold,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Cut at {y_threshold}\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Hierarchical Clustering on {len(df_centroids)} Centroids\\n(Ward Linkage)\",\n",
    "    fontsize=16\n",
    ")\n",
    "plt.xlabel(\"Combination (behavior_cluster, profile_cluster)\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26275fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cluster based on dendrogram cut\n",
    "n_final_clusters = 6\n",
    "\n",
    "hclust_final = AgglomerativeClustering(\n",
    "    linkage='ward',\n",
    "    n_clusters=n_final_clusters,\n",
    ")\n",
    "\n",
    "hclust_final_labels = hclust_final.fit_predict(df_centroids)\n",
    "\n",
    "df_centroids['merged_labels'] = hclust_final_labels\n",
    "\n",
    "cluster_mapper = df_centroids['merged_labels'].to_dict()\n",
    "\n",
    "final_df['merged_labels'] = final_df.apply(\n",
    "    lambda row: cluster_mapper[(row['behavior_cluster'], row['profile_cluster'])],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final clusters on UMAP\n",
    "umap_final_df = apply_umap_2d(final_df[combined_features])\n",
    "pca_final_df = apply_pca_2d(final_df[combined_features])\n",
    "tsne_final_df = apply_tsne_2d(final_df[combined_features])\n",
    "\n",
    "plot_cluster(\n",
    "    umap_final_df, pca_final_df, tsne_final_df, final_df['merged_labels'],\n",
    "    main_title=\"Final Merged Clusters on Combined Features\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cf6fb",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts and percentages\n",
    "cluster_counts = final_df['merged_labels'].value_counts().sort_index()\n",
    "cluster_pcts = (final_df['merged_labels'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "# Create the Bar Chart Figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    x=cluster_counts.index, \n",
    "    y=cluster_counts.values, \n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Cluster Size Distribution\\nVolume of Customers per Segment', fontsize=14)\n",
    "plt.xlabel('Merged Cluster ID', fontsize=12)\n",
    "plt.ylabel('Number of Customers', fontsize=12)\n",
    "\n",
    "# Annotate bars with counts and percentages\n",
    "for i, count in enumerate(cluster_counts.values):\n",
    "    pct = cluster_pcts.values[i]\n",
    "    ax.text(\n",
    "        i, count + (max(cluster_counts)*0.02), \n",
    "        f'{count}\\n({pct:.1f}%)', \n",
    "        ha='center', va='bottom', fontweight='bold', fontsize=10\n",
    "    )\n",
    "\n",
    "plt.ylim(0, max(cluster_counts) * 1.15)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ed2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the Centroids (Means) for each cluster\n",
    "df_heatmap = final_df.groupby('merged_labels')[combined_features].mean().T\n",
    "\n",
    "# Create the Heatmap Figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    df_heatmap, \n",
    "    annot=True,\n",
    "    fmt=\".2f\", \n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Standardized Value (Z-Score)'}\n",
    ")\n",
    "\n",
    "plt.title('Cluster Profiling Heatmap\\nFeature Deviations by Segment', fontsize=16)\n",
    "plt.xlabel('Merged Cluster ID', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bring the labels into the dataframe that has the lifecycle columns\n",
    "model_df['merged_labels'] = final_df['merged_labels']\n",
    "\n",
    "# 2. Group and calculate the means\n",
    "df_lifecycle = model_df.groupby('merged_labels')[lifecycle_features].mean().T\n",
    "\n",
    "# 3. Create the Heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    df_lifecycle, \n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='RdBu_r',       \n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Average Value'}\n",
    ")\n",
    "\n",
    "plt.title('Customer Lifecycle Profile by Segment', fontsize=14)\n",
    "plt.xlabel('Merged Cluster ID', fontsize=12)\n",
    "plt.ylabel('Lifecycle Features', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a6cf48",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis\n",
    "\n",
    "Use Random Forest Classifier to identify which features are most important for predicting the merged cluster labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6412f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier on combined features with merged labels\n",
    "X_train = final_df[combined_features]\n",
    "y_train = final_df['merged_labels']\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': combined_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['Importance'].values, color='steelblue')\n",
    "\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(top_features)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'].values)\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_title(f'Feature Importance for Merged Cluster\\n(Random Forest Classifier)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (idx, row) in enumerate(top_features.iterrows()):\n",
    "    ax.text(row['Importance'], i, f\" {row['Importance']:.4f}\", va='center', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Feature Importance Plot\n",
    "cumulative_importance = np.cumsum(feature_importance_df['Importance'].values)\n",
    "cumulative_percentage = (cumulative_importance / cumulative_importance[-1]) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(range(1, len(cumulative_percentage) + 1), cumulative_percentage, \n",
    "        marker='o', linestyle='-', linewidth=2, markersize=4, color='steelblue')\n",
    "\n",
    "ax.axhline(y=80, color='red', linestyle='--', linewidth=2, label='80% Threshold')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% Threshold')\n",
    "\n",
    "# Find number of features for 80% and 90% importance\n",
    "n_features_80 = np.argmax(cumulative_percentage >= 80) + 1\n",
    "n_features_90 = np.argmax(cumulative_percentage >= 90) + 1\n",
    "\n",
    "ax.axvline(x=n_features_80, color='red', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=n_features_90, color='orange', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Number of Features (Ranked by Importance)', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Importance (%)', fontsize=12)\n",
    "ax.set_title('Cumulative Feature Importance\\n(Random Forest Classifier)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "ax.text(n_features_80, 80, f'\\n  {n_features_80} features', fontsize=10, color='red', fontweight='bold')\n",
    "ax.text(n_features_90, 90, f'\\n  {n_features_90} features', fontsize=10, color='orange', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNumber of features needed for 80% importance: {n_features_80}\")\n",
    "print(f\"Number of features needed for 90% importance: {n_features_90}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance by Category (Behavior vs Profile)\n",
    "behavior_importance = feature_importance_df[feature_importance_df['Feature'].isin(behavior_features)].copy()\n",
    "profile_importance = feature_importance_df[feature_importance_df['Feature'].isin(profile_features)].copy()\n",
    "\n",
    "# Calculate aggregate importance by category\n",
    "category_importance = pd.DataFrame({\n",
    "    'Category': ['Behavior Features', 'Profile Features'],\n",
    "    'Total Importance': [\n",
    "        behavior_importance['Importance'].sum(),\n",
    "        profile_importance['Importance'].sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors_pie = ['#66c2a5', '#fc8d62']\n",
    "axes[0].pie(category_importance['Total Importance'], \n",
    "            labels=category_importance['Category'], \n",
    "            autopct='%1.1f%%',\n",
    "            colors=colors_pie,\n",
    "            startangle=90,\n",
    "            textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "axes[0].set_title('Importance Distribution by Feature Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Bar chart comparing top features from each category\n",
    "top_behavior = behavior_importance.head(10)\n",
    "top_profile = profile_importance.head(10)\n",
    "\n",
    "# Use the maximum number of features across both categories\n",
    "max_features = max(len(top_behavior), len(top_profile))\n",
    "y_pos = np.arange(max_features)\n",
    "width = 0.35\n",
    "\n",
    "# Pad with zeros if one has fewer features than the other\n",
    "behavior_values = np.pad(top_behavior['Importance'].values[::-1], (0, max_features - len(top_behavior)), mode='constant')\n",
    "profile_values = np.pad(top_profile['Importance'].values[::-1], (0, max_features - len(top_profile)), mode='constant')\n",
    "\n",
    "axes[1].barh(y_pos - width/2, behavior_values, width, \n",
    "             label='Behavior', color='#66c2a5', alpha=0.8)\n",
    "axes[1].barh(y_pos + width/2, profile_values, width, \n",
    "             label='Profile', color='#fc8d62', alpha=0.8)\n",
    "\n",
    "axes[1].set_yticks(y_pos)\n",
    "axes[1].set_xlabel('Importance Score', fontsize=11)\n",
    "axes[1].set_title('Top Features by Category', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nImportance Summary by Category:\")\n",
    "print(category_importance)\n",
    "print(f\"\\nTop 5 Behavior Features:\")\n",
    "print(behavior_importance.head(5))\n",
    "print(f\"\\nTop 5 Profile Features:\")\n",
    "print(profile_importance.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d4bb2",
   "metadata": {},
   "source": [
    "# Assing Labels to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_labels = rf_model.predict(outliers_df[combined_features])\n",
    "outliers_df['predicted_merged_labels'] = out_labels\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='predicted_merged_labels', data=outliers_df, color='steelblue', alpha=0.8)\n",
    "plt.title('Predicted Cluster Distribution for Outliers', fontsize=14)\n",
    "plt.xlabel('Predicted Merged Cluster ID', fontsize=12)\n",
    "plt.ylabel('Number of Outlier Customers', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

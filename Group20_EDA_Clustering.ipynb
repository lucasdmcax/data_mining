{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642b95d6",
   "metadata": {},
   "source": [
    "# Group 20 — Exploratory Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on customer and flights databases, covering data inspection, quality assessment, and preprocessing.\n",
    "\n",
    "## Table of Contents\n",
    "- [Data Import](#data-import)\n",
    "- [Data Preprocessing](#preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bb2db",
   "metadata": {},
   "source": [
    "# <a id=\"data-import\"></a> Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0215ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the data\n",
    "\n",
    "customer_db = pd.read_csv(\"data/DM_AIAI_CustomerDB.csv\", index_col=0 )\n",
    "flights_db = pd.read_csv(\"data/DM_AIAI_FlightsDB.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfe1cb",
   "metadata": {},
   "source": [
    "# <a id=\"preprocessing\"></a> Data Preprocessing\n",
    "\n",
    "In this section we apply the preprocessing and feature engineering steps\n",
    "described in the report:\n",
    "\n",
    "- Fix data types and logical inconsistencies in **FlightsDB**\n",
    "- Handle missing values and transform skewed variables in **CustomerDB**\n",
    "- Create new features: log-transformed variables, points utilisation,\n",
    "  cancellation flag, customer value score, flight activity score, and\n",
    "  average flight distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_flights(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the FlightsDB:\n",
    "    - Convert YearMonthDate to datetime\n",
    "    - Round down NumFlights and NumFlightsWithCompanions\n",
    "    - Set DistanceKM = 0 where NumFlights == 0\n",
    "    - Drop DollarCostPointsRedeemed\n",
    "    - Add log-transformed versions of skewed variables\n",
    "    - Create PointsUtilizationRatio = PointsRedeemed / PointsAccumulated\n",
    "    \"\"\"\n",
    "    df = flights_df.copy()\n",
    "\n",
    "    # 1. YearMonthDate -> datetime\n",
    "    if 'YearMonthDate' in df.columns:\n",
    "        df['YearMonthDate'] = pd.to_datetime(df['YearMonthDate'])\n",
    "\n",
    "    # 2. Round down flight counts and cast to int\n",
    "    for col in ['NumFlights', 'NumFlightsWithCompanions']:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.floor(df[col]).astype(int)\n",
    "\n",
    "    # 3. Fix logical inconsistency: DistanceKM must be 0 if NumFlights == 0\n",
    "    if {'NumFlights', 'DistanceKM'}.issubset(df.columns):\n",
    "        df.loc[df['NumFlights'] == 0, 'DistanceKM'] = 0\n",
    "\n",
    "    # 4. Drop perfectly correlated variable\n",
    "    if 'DollarCostPointsRedeemed' in df.columns:\n",
    "        df = df.drop(columns=['DollarCostPointsRedeemed'])\n",
    "\n",
    "    # 5. Log transforms for skewed numeric variables\n",
    "    log_cols = ['DistanceKM', 'PointsAccumulated', 'PointsRedeemed']\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # 6. Points utilisation ratio\n",
    "    if {'PointsRedeemed', 'PointsAccumulated'}.issubset(df.columns):\n",
    "        denom = df['PointsAccumulated'].replace({0: np.nan})\n",
    "        df['PointsUtilizationRatio'] = df['PointsRedeemed'] / denom\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_customers(customer_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the CustomerDB:\n",
    "    - Create cancellation flag from CancellationDate\n",
    "    - Group-median imputation (by LoyaltyStatus) for Income and Customer Lifetime Value\n",
    "    - Log transform Customer Lifetime Value\n",
    "    - Create Location feature (region) from Province or State (placeholder mapping)\n",
    "    - Create CustomerValueScore composite feature\n",
    "    \"\"\"\n",
    "    df = customer_df.copy()\n",
    "\n",
    "    # 1. Cancellation flag\n",
    "    if 'CancellationDate' in df.columns:\n",
    "        df['CancelledFlag'] = df['CancellationDate'].notna().astype(int)\n",
    "\n",
    "    # 2. Group-median imputation by LoyaltyStatus\n",
    "    group_col = 'LoyaltyStatus'\n",
    "    cols_to_impute = ['Income', 'Customer Lifetime Value']\n",
    "    for col in cols_to_impute:\n",
    "        if col in df.columns and group_col in df.columns:\n",
    "            df[col] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "\n",
    "    # 3. Log transform Customer Lifetime Value (for variance stabilisation)\n",
    "    if 'Customer Lifetime Value' in df.columns:\n",
    "        df['CustomerLifetimeValue_log'] = np.log1p(df['Customer Lifetime Value'])\n",
    "\n",
    "    # 4. Location feature (region mapping) – fill mapping as desired\n",
    "    if 'Province or State' in df.columns:\n",
    "        region_map = {\n",
    "            'Ontario': 'Central',\n",
    "            'Quebec': 'Quebec',\n",
    "            'British Columbia': 'West',\n",
    "            'Alberta': 'West',\n",
    "            'Saskatchewan': 'West',\n",
    "            'Manitoba': 'West',\n",
    "            'New Brunswick': 'Atlantic',\n",
    "            'Nova Scotia': 'Atlantic',\n",
    "            'Prince Edward Island': 'Atlantic',\n",
    "            'Newfoundland and Labrador': 'Atlantic',\n",
    "            'Yukon': 'North',\n",
    "            'Northwest Territories': 'North',\n",
    "            'Nunavut': 'North',\n",
    "        }\n",
    "\n",
    "        df['Location'] = df['Province or State'].map(region_map).fillna('Other')\n",
    "\n",
    "    # 5. Customer Value Score (simple composite of CLV and Income)\n",
    "    clv_col = 'CustomerLifetimeValue_log'\n",
    "    if clv_col in df.columns:\n",
    "        clv_scaled = (df[clv_col] - df[clv_col].mean()) / df[clv_col].std(ddof=0)\n",
    "\n",
    "        if 'Income' in df.columns:\n",
    "            income_log = np.log1p(df['Income'].clip(lower=0))\n",
    "            income_scaled = (income_log - income_log.mean()) / income_log.std(ddof=0)\n",
    "            # Heavier weight on CLV, lighter on Income\n",
    "            df['CustomerValueScore'] = 0.7 * clv_scaled + 0.3 * income_scaled\n",
    "        else:\n",
    "            df['CustomerValueScore'] = clv_scaled\n",
    "\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be98085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_customer_flight_features(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate monthly flight records into customer-level features:\n",
    "    - TotalFlights, TotalDistanceKM, TotalPointsAccumulated, TotalPointsRedeemed\n",
    "    - MeanPointsUtilization\n",
    "    - AverageFlightDistance\n",
    "    - FlightActivityScore (based on z-scores of flights and distance)\n",
    "    \"\"\"\n",
    "    id_col = 'Loyalty#'\n",
    "    df = flights_df.copy()\n",
    "\n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(id_col)\n",
    "        .agg(\n",
    "            TotalFlights=('NumFlights', 'sum'),\n",
    "            TotalDistanceKM=('DistanceKM', 'sum'),\n",
    "            TotalPointsAccumulated=('PointsAccumulated', 'sum'),\n",
    "            TotalPointsRedeemed=('PointsRedeemed', 'sum'),\n",
    "            MeanPointsUtilization=('PointsUtilizationRatio', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Average flight distance = total distance / total flights\n",
    "    agg['AverageFlightDistance'] = agg['TotalDistanceKM'] / agg['TotalFlights'].replace({0: np.nan})\n",
    "\n",
    "    # FlightActivityScore: combines total flights and distance (z-scores)\n",
    "    for col in ['TotalFlights', 'TotalDistanceKM']:\n",
    "        mean = agg[col].mean()\n",
    "        std = agg[col].std(ddof=0)\n",
    "        if std == 0:\n",
    "            agg[f'{col}_z'] = 0\n",
    "        else:\n",
    "            agg[f'{col}_z'] = (agg[col] - mean) / std\n",
    "\n",
    "    agg['FlightActivityScore'] = agg['TotalFlights_z'] + agg['TotalDistanceKM_z']\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing for each table\n",
    "customer_preprocessed = preprocess_customers(customer_db)\n",
    "flights_preprocessed = preprocess_flights(flights_db)\n",
    "\n",
    "# Build customer-level flight features\n",
    "customer_flight_features = build_customer_flight_features(flights_preprocessed)\n",
    "\n",
    "# Merge into a single modelling dataset (one row per customer)\n",
    "id_col = 'Loyalty#'\n",
    "model_df = (\n",
    "    customer_preprocessed\n",
    "    .merge(customer_flight_features, on=id_col, how='left')\n",
    ")\n",
    "\n",
    "print(\"Customer-preprocessed shape:\", customer_preprocessed.shape)\n",
    "print(\"Flights-preprocessed shape:\", flights_preprocessed.shape)\n",
    "print(\"Model dataset shape:\", model_df.shape)\n",
    "\n",
    "model_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9406aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def winsorize_dataframe(df, columns, limits=(0.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Apply winsorization to each column in `columns`.\n",
    "    limits=(lower_pct, upper_pct) means: cap values at the 1st and 99th percentile.\n",
    "\n",
    "    Returns the winsorized copy of df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # winsorize returns masked arrays -> convert to normal array\n",
    "            df[col] = winsorize(df[col], limits=limits).data\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_outlier_cols = [\n",
    "    'TotalFlights',\n",
    "    'TotalDistanceKM',\n",
    "    'TotalPointsAccumulated',\n",
    "    'TotalPointsRedeemed',\n",
    "    'AverageFlightDistance'\n",
    "]\n",
    "\n",
    "customer_flight_features_wins = winsorize_dataframe(\n",
    "    customer_flight_features,\n",
    "    columns=flight_outlier_cols,\n",
    "    limits=(0.01, 0.01)    # winsorize at 1% and 99%\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_outlier_cols = [\n",
    "    'Income',\n",
    "    'Customer Lifetime Value',\n",
    "    'CustomerLifetimeValue_log'\n",
    "]\n",
    "\n",
    "customer_preprocessed_wins = winsorize_dataframe(\n",
    "    customer_preprocessed,\n",
    "    columns=customer_outlier_cols,\n",
    "    limits=(0.01, 0.01)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'Loyalty#'\n",
    "\n",
    "model_df = (\n",
    "    customer_preprocessed_wins\n",
    "    .merge(customer_flight_features_wins, on=id_col, how='left')\n",
    ")\n",
    "\n",
    "print(\"Final model_df shape:\", model_df.shape)\n",
    "model_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining missing values\n",
    "model_df.isna().sum().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop CancellationDate (flag already exists)\n",
    "model_df = model_df.drop(columns=['CancellationDate'], errors='ignore')\n",
    "\n",
    "# Fill average distance for zero-flight customers\n",
    "model_df['AverageFlightDistance'] = model_df['AverageFlightDistance'].fillna(0)\n",
    "\n",
    "# Fill points utilization for customers with no point activity\n",
    "model_df['MeanPointsUtilization'] = model_df['MeanPointsUtilization'].fillna(0)\n",
    "\n",
    "# Fill ALL flight-related NaNs with 0\n",
    "flight_cols = [\n",
    "    'TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated',\n",
    "    'TotalPointsRedeemed', 'MeanPointsUtilization', 'AverageFlightDistance',\n",
    "    'TotalFlights_z', 'TotalDistanceKM_z', 'FlightActivityScore'\n",
    "]\n",
    "\n",
    "for col in flight_cols:\n",
    "    model_df[col] = model_df[col].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features for clustering\n",
    "numeric_features = [\n",
    "    'Income',\n",
    "    'CustomerLifetimeValue_log',\n",
    "    'CustomerValueScore',\n",
    "    \n",
    "    'TotalFlights',\n",
    "    'TotalDistanceKM',\n",
    "    'AverageFlightDistance',\n",
    "    'TotalPointsAccumulated',\n",
    "    'TotalPointsRedeemed',\n",
    "    'MeanPointsUtilization',\n",
    "    'FlightActivityScore',\n",
    "\n",
    "    'CancelledFlag'\n",
    "]\n",
    "\n",
    "# Categorical features to encode\n",
    "categorical_features = [\n",
    "    'EnrollmentType',  # Bronze, Silver, Gold ...\n",
    "    'Location'         # Region/Province after mapping\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed99397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = model_df[numeric_features + categorical_features].copy()\n",
    "df_selected.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e15d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_selected, columns=categorical_features, drop_first=True)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6296ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "X_scaled[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7e97d",
   "metadata": {},
   "source": [
    "# DBSCAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3419c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_dbscan_outliers(df: pd.DataFrame, features: list, eps=1.9, min_samples=20):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN to identify outliers in the dataset.\n",
    "    Returns a DataFrame with original data and DBSCAN labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "    labels = dbscan.fit_predict(df[features])\n",
    "    outlier_count = Counter(labels)\n",
    "    print(f\"DBSCAN results: {outlier_count}\")\n",
    "    print(f\"Outliers detected: {outlier_count[-1]}\")\n",
    "    print(f\"Core customers: {outlier_count[0]}\")\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    df_out['DBSCAN_Label'] = labels\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2802c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = get_dbscan_outliers(df_selected, numeric_features, eps=1.9, min_samples=20)\n",
    "df_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers['DBSCAN_Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec503e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ff7fe",
   "metadata": {},
   "source": [
    " Given the fact that -1 represents the outliers, than there are two clusters (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1184719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. Distribution:\")\n",
    "\n",
    "for label in sorted(set(labels)):\n",
    "    count = sum(1 for x in labels if x == label)\n",
    "    pct = count / len(labels) * 100\n",
    "    kind = \"OUTLIERS\" if label == -1 else f\"Cluster {label}\"\n",
    "    print(f\"   {kind:15} → {count:5} points ({pct:5.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b6a4a",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

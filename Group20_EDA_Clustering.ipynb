{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642b95d6",
   "metadata": {},
   "source": [
    "# Customer Clustering: Multi-Perspective Approach\n",
    "\n",
    "This notebook identifies customer clusters from different perspectives (behavioral, profile), compares clustering algorithms, and merges the best solutions using hierarchical clustering on centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans, MeanShift, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca93a",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "All custom functions for preprocessing, feature engineering, and aggregation are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_dataframe(df, columns, limits=(0.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Apply winsorization to each column in `columns`.\n",
    "    limits=(lower_pct, upper_pct) means: cap values at the 1st and 99th percentile.\n",
    "\n",
    "    Returns the winsorized copy of df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # winsorize returns masked arrays -> convert to normal array\n",
    "            df[col] = winsorize(df[col], limits=limits).data\n",
    "    return df\n",
    "\n",
    "def preprocess_flights(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the FlightsDB:\n",
    "    - Winsorize outliers\n",
    "    - Convert YearMonthDate to datetime\n",
    "    - Round down NumFlights and NumFlightsWithCompanions\n",
    "    - Set DistanceKM = 0 where NumFlights == 0\n",
    "    - Drop DollarCostPointsRedeemed\n",
    "    - Add log-transformed versions of skewed variables\n",
    "    - Create PointsUtilizationRatio = PointsRedeemed / PointsAccumulated\n",
    "    \"\"\"\n",
    "    df = flights_df.copy()\n",
    "\n",
    "    # 0. Winsorize outliers (Flights DB outliers are legitimate but skewed)\n",
    "    outlier_cols = [\n",
    "        'NumFlights', 'NumFlightsWithCompanions', 'DistanceKM', \n",
    "        'PointsAccumulated', 'PointsRedeemed'\n",
    "    ]\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 1. YearMonthDate -> datetime\n",
    "    if 'YearMonthDate' in df.columns:\n",
    "        df['YearMonthDate'] = pd.to_datetime(df['YearMonthDate'])\n",
    "\n",
    "    # 2. Round down flight counts and cast to int\n",
    "    for col in ['NumFlights', 'NumFlightsWithCompanions']:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.floor(df[col]).astype(int)\n",
    "\n",
    "    # 3. Fix logical inconsistency: DistanceKM must be 0 if NumFlights == 0\n",
    "    if {'NumFlights', 'DistanceKM'}.issubset(df.columns):\n",
    "        df.loc[df['NumFlights'] == 0, 'DistanceKM'] = 0\n",
    "\n",
    "    # 4. Drop perfectly correlated variable\n",
    "    if 'DollarCostPointsRedeemed' in df.columns:\n",
    "        df = df.drop(columns=['DollarCostPointsRedeemed'])\n",
    "\n",
    "    # 5. Log transforms for skewed numeric variables\n",
    "    log_cols = ['DistanceKM', 'PointsAccumulated', 'PointsRedeemed', 'NumFlights']\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # 6. Points utilisation ratio\n",
    "    if {'PointsRedeemed', 'PointsAccumulated'}.issubset(df.columns):\n",
    "        denom = df['PointsAccumulated'].replace({0: np.nan})\n",
    "        df['PointsUtilizationRatio'] = df['PointsRedeemed'] / denom\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_customers(customer_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the CustomerDB:\n",
    "    - Create cancellation flag from CancellationDate\n",
    "    - Group-median imputation (by LoyaltyStatus) for Income and Customer Lifetime Value\n",
    "    - Winsorize outliers (Income, CLV)\n",
    "    - Log transform Customer Lifetime Value and Income\n",
    "    - Encode Gender as binary\n",
    "    \"\"\"\n",
    "    df = customer_df.copy()\n",
    "\n",
    "    # 1. Cancellation flag\n",
    "    if 'CancellationDate' in df.columns:\n",
    "        df['CancelledFlag'] = df['CancellationDate'].notna().astype(int)\n",
    "\n",
    "    # 2. Group-median imputation by LoyaltyStatus\n",
    "    group_col = 'LoyaltyStatus'\n",
    "    cols_to_impute = ['Income', 'Customer Lifetime Value']\n",
    "    for col in cols_to_impute:\n",
    "        if col in df.columns and group_col in df.columns:\n",
    "            df[col] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "\n",
    "    # 3. Winsorize outliers\n",
    "    outlier_cols = ['Income', 'Customer Lifetime Value']\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 4. Log transforms\n",
    "    if 'Customer Lifetime Value' in df.columns:\n",
    "        df['CLV_log'] = np.log1p(df['Customer Lifetime Value'])\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'].clip(lower=0))\n",
    "\n",
    "    # 5. Gender encoding\n",
    "    if 'Gender' in df.columns:\n",
    "        df['Gender'] = df['Gender'].map({'female': 1, 'male': 0}).fillna(0).astype(int)\n",
    "\n",
    "    # 6. Education to Years (Ordinal Encoding)\n",
    "    if 'Education' in df.columns:\n",
    "        edu_map = {\n",
    "            'High School or Below': 12,\n",
    "            'College': 14,\n",
    "            'Bachelor': 16,\n",
    "            'Master': 18,\n",
    "            'Doctor': 21\n",
    "        }\n",
    "        df['Education'] = df['Education'].map(edu_map)\n",
    "        df['Education'] = df['Education'].fillna(16)\n",
    "\n",
    "    # 7. Turn marital status into a flag\n",
    "    if 'Marital Status' in df.columns:\n",
    "        df['Marital Status'] = np.where(df['Marital Status'] != 'Married', 1, 0)\n",
    "\n",
    "    # 8. Tenure\n",
    "    ref_date = pd.to_datetime('2022-01-01')\n",
    "    if 'EnrollmentDateOpening' in df.columns:\n",
    "        df['EnrollmentDateOpening'] = pd.to_datetime(df['EnrollmentDateOpening'])\n",
    "        df['TenureMonths'] = (ref_date - df['EnrollmentDateOpening']) / pd.Timedelta(days=30.44)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_customer_flight_features(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate monthly flight records into customer-level features:\n",
    "    - TotalFlights, TotalDistanceKM, TotalPointsAccumulated, TotalPointsRedeemed\n",
    "    - MeanPointsUtilization\n",
    "    - AverageFlightDistance\n",
    "    \"\"\"\n",
    "    id_col = 'Loyalty#'\n",
    "    df = flights_df.copy()\n",
    "    \n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(id_col)\n",
    "        .agg(\n",
    "            TotalFlights=('NumFlights', 'sum'),\n",
    "            TotalDistanceKM=('DistanceKM', 'sum'),\n",
    "            TotalPointsAccumulated=('PointsAccumulated', 'sum'),\n",
    "            TotalPointsRedeemed=('PointsRedeemed', 'sum'),\n",
    "            MeanPointsUtilization=('PointsUtilizationRatio', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Log transforms for aggregated features\n",
    "    for col in ['TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed']:\n",
    "        agg[f'{col}_log'] = np.log1p(agg[col])\n",
    "    \n",
    "    # Average flight distance\n",
    "    agg['AverageFlightDistance'] = agg['TotalDistanceKM'] / agg['TotalFlights'].replace({0: np.nan})\n",
    "\n",
    "    return agg\n",
    "\n",
    "def create_model_df(customer_df: pd.DataFrame, flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the final modeling dataframe:\n",
    "    1. Preprocess customers and flights\n",
    "    2. Build customer-level flight features\n",
    "    3. Merge datasets (Left Join)\n",
    "    4. Set Loyalty# as Index\n",
    "    5. Handle missing values\n",
    "    6. Encode categorical variables (OneHotEncoder)\n",
    "    7. Drop unnecessary columns\n",
    "    8. Scale numeric features (StandardScaler)\n",
    "    \"\"\"\n",
    "    # 1. Preprocess\n",
    "    cust_clean = preprocess_customers(customer_df)\n",
    "    flights_clean = preprocess_flights(flights_df)\n",
    "\n",
    "    # 2. Build flight features\n",
    "    flight_features = build_customer_flight_features(flights_clean)\n",
    "\n",
    "    # 3. Merge\n",
    "    model_df = cust_clean.merge(flight_features, on='Loyalty#', how='left')\n",
    "\n",
    "    # 4. Set Loyalty# as Index\n",
    "    if 'Loyalty#' in model_df.columns:\n",
    "        model_df.set_index('Loyalty#', inplace=True)\n",
    "\n",
    "    # 5. Handle Missing Values (Numeric)\n",
    "    numeric_cols_to_fill = model_df.select_dtypes(include=[np.number]).columns\n",
    "    model_df[numeric_cols_to_fill] = model_df[numeric_cols_to_fill].fillna(0)\n",
    "\n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'First Name', 'Last Name', 'CancellationDate', 'Customer Name',\n",
    "        'Country', 'Province or State', 'City', 'Postal Code',\n",
    "        'Latitude', 'Longitude', 'EnrollmentDateOpening', 'EnrollmentType',\n",
    "        'TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed',\n",
    "        'Customer Lifetime Value', 'Income'\n",
    "    ]\n",
    "    model_df = model_df.drop(columns=[c for c in cols_to_drop if c in model_df.columns], errors='ignore')\n",
    "\n",
    "    # 7. Separate Numeric and Categorical\n",
    "    categorical_cols = ['LoyaltyStatus', 'Location Code']\n",
    "    categorical_cols = [c for c in categorical_cols if c in model_df.columns]\n",
    "       \n",
    "    numeric_cols = model_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude binary/ordinal from scaling\n",
    "    unscaled_cols = []\n",
    "    for col in ['CancelledFlag', 'Marital Status', 'Gender']:\n",
    "        if col in numeric_cols:\n",
    "            numeric_cols.remove(col)\n",
    "            unscaled_cols.append(col)\n",
    "\n",
    "    # 8. OneHotEncoding\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first', dtype=int)\n",
    "    encoded_data = ohe.fit_transform(model_df[categorical_cols])\n",
    "    encoded_cols = ohe.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=encoded_cols, index=model_df.index)\n",
    "    \n",
    "    # 9. Scale Numeric Features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_numeric = scaler.fit_transform(model_df[numeric_cols])\n",
    "    df_numeric_scaled = pd.DataFrame(scaled_numeric, columns=numeric_cols, index=model_df.index)\n",
    "    \n",
    "    # 10. Combine\n",
    "    dfs_to_concat = [df_numeric_scaled, df_encoded]\n",
    "    if unscaled_cols:\n",
    "        dfs_to_concat.append(model_df[unscaled_cols])\n",
    "        \n",
    "    df_final = pd.concat(dfs_to_concat, axis=1)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def evaluate_clustering(algorithm_cls, X, param_name, param_values):\n",
    "    \"\"\"\n",
    "    Evaluate clustering algorithm with multiple metrics.\n",
    "    Prints number of clusters, Silhouette, Davies-Bouldin, and R2 metrics (rounded to 2 decimals).\n",
    "    \"\"\"\n",
    "    algo_name = algorithm_cls.__name__\n",
    "    print(f\"\\n{algo_name} Clustering Results:\")\n",
    "    for param in param_values:\n",
    "        model = algorithm_cls(**{param_name: param})\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(set(labels))\n",
    "        sil = silhouette_score(X, labels) if n_clusters > 1 else None\n",
    "        db = davies_bouldin_score(X, labels) if n_clusters > 1 else None\n",
    "        # R2 metric: ratio of between-cluster variance to total variance\n",
    "        if n_clusters > 1:\n",
    "            cluster_means = np.array([X[labels == k].mean(axis=0) for k in set(labels)])\n",
    "            overall_mean = X.mean(axis=0)\n",
    "            between_var = sum([(len(X[labels == k]) * np.sum((cluster_means[i] - overall_mean) ** 2)) for i, k in enumerate(set(labels))])\n",
    "            total_var = np.sum((X - overall_mean) ** 2)\n",
    "            r2 = between_var / total_var if total_var > 0 else None\n",
    "        else:\n",
    "            r2 = None\n",
    "        # Round metrics to 2 decimals if not None\n",
    "        sil_str = f\"{sil:.2f}\" if sil is not None else \"NA\"\n",
    "        db_str = f\"{db:.2f}\" if db is not None else \"NA\"\n",
    "        r2_str = f\"{r2:.2f}\" if r2 is not None else \"NA\"\n",
    "        print(f\"{algo_name} ({param_name}={param}): Clusters={n_clusters}, Silhouette={sil_str}, Davies-Bouldin={db_str}, R2={r2_str}\")\n",
    "\n",
    "def apply_pca(X, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(f\"PCA reduced to {X_pca.shape[1]} components, explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e363a",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "\n",
    "Load customer and flight data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ceb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "customer_db = pd.read_csv(\"data/DM_AIAI_CustomerDB.csv\", index_col=0 )\n",
    "flights_db = pd.read_csv(\"data/DM_AIAI_FlightsDB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfe1cb",
   "metadata": {},
   "source": [
    "# Feature Set Definition\n",
    "\n",
    "Define behavioral and profile feature sets for clustering perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257255fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in customer database\n",
    "initial_rows = customer_db.shape[0]\n",
    "duplicated_loyalty_ids = customer_db[customer_db['Loyalty#'].duplicated()]['Loyalty#'].unique()\n",
    "customer_db = customer_db.drop_duplicates(subset=['Loyalty#'])\n",
    "dropped_rows = initial_rows - customer_db.shape[0]\n",
    "dropped_percentage = (dropped_rows / initial_rows) * 100\n",
    "\n",
    "print(f\"Dropped {dropped_rows} duplicate customers ({dropped_percentage:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the modeling dataset using the pipeline function\n",
    "# This handles preprocessing, merging, missing values, encoding, feature selection AND scaling\n",
    "model_df = create_model_df(customer_db, flights_db)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7666a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories for profile and behavior segmentation\n",
    "behavior_features = [\n",
    "    'TotalFlights_log', 'TotalDistanceKM_log', 'TotalPointsAccumulated_log', \n",
    "    'TotalPointsRedeemed_log', 'MeanPointsUtilization', 'AverageFlightDistance', \n",
    "]\n",
    "\n",
    "profile_features = [\n",
    "    'CLV_log', 'Income_log', 'Gender', 'Education', 'Marital Status',\n",
    "    'LoyaltyStatus_Nova', 'LoyaltyStatus_Star', \n",
    "    'Location Code_Suburban', 'Location Code_Urban', 'TenureMonths',\n",
    "    'CancelledFlag'\n",
    "]\n",
    "\n",
    "print(f\"Behavior features: {behavior_features}\")\n",
    "print(f\"Profile features: {profile_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the model dataframe\n",
    "model_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208e24c",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "Analyze and remove highly correlated features to improve clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4707b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Feature Selection\n",
    "# 1. Plot initial correlation matrix\n",
    "plt.figure(figsize=(24, 20))\n",
    "initial_corr = model_df.corr()\n",
    "sns.heatmap(initial_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Initial Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 2. Identify and remove highly correlated features (> 0.8)\n",
    "# We use the absolute correlation matrix\n",
    "corr_matrix = model_df.corr().abs()\n",
    "\n",
    "# We select the upper triangle of the correlation matrix. \n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# We drop the column (Feature B) if it has a correlation > 0.8 with any previous column (Feature A).\n",
    "# This way, Feature A is kept and Feature B is removed.\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "print(f\"Dropping highly correlated features: {to_drop}\")\n",
    "\n",
    "# Drop features from model_df\n",
    "model_df = model_df.drop(columns=to_drop)\n",
    "\n",
    "# Update feature lists to remove dropped columns\n",
    "behavior_features = [f for f in behavior_features if f not in to_drop]\n",
    "profile_features = [f for f in profile_features if f not in to_drop]\n",
    "\n",
    "print(f\"Updated behavior features: {behavior_features}\")\n",
    "print(f\"Updated profile features: {profile_features}\")\n",
    "\n",
    "# 3. Plot updated correlation matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(model_df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing Highly Correlated Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7e97d",
   "metadata": {},
   "source": [
    "# Outlier Detection\n",
    "\n",
    "Detect and remove multivariate outliers using DBSCAN before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4061e8",
   "metadata": {},
   "source": [
    "DBSCAN is applied to the scaled feature space for outlier detection. In this project, DBSCAN is used exclusively for identifying anomalous customers, not for final segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3419c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model_df as X_scaled (Loyalty# is now the index)\n",
    "dbscan = DBSCAN(eps=1.9, min_samples=20, n_jobs=1)\n",
    "dbscan_labels = dbscan.fit_predict(model_df)\n",
    "\n",
    "outlier_count = Counter(dbscan_labels)\n",
    "print(f\"DBSCAN results: {outlier_count}\")\n",
    "print(f\"Outliers detected: {outlier_count.get(-1, 0)}\")\n",
    "print(f\"Core customers: {outlier_count.get(0, 0)}\")\n",
    "\n",
    "core_mask = (dbscan_labels != -1)\n",
    "\n",
    "model_df_clipped = model_df[core_mask]\n",
    "outliers_df = model_df[dbscan_labels == -1]\n",
    "\n",
    "print(f\"Core customers kept: {len(model_df_clipped):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b6a4a",
   "metadata": {},
   "source": [
    "# Clustering Algorithms Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eb7fd",
   "metadata": {},
   "source": [
    "## Clustering: Multiple Algorithms & Perspectives\n",
    "\n",
    "We now test several clustering algorithms (KMeans, DBSCAN, MeanShift) with different hyperparameters, This approach allows us to compare cluster quality and interpretability across perspectives and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature sets for clustering\n",
    "behavior_df = model_df_clipped[behavior_features]\n",
    "profile_df = model_df_clipped[profile_features]\n",
    "\n",
    "# Apply PCA to reduce dimensionality for each feature set\n",
    "\n",
    "behavior_df_pca = apply_pca(behavior_df.values)\n",
    "profile_df_pca = apply_pca(profile_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on behavior features\n",
    "kmeans_params = [2, 3, 4, 5, 6]\n",
    "evaluate_clustering(KMeans, behavior_df_pca, 'n_clusters', kmeans_params)\n",
    "\n",
    "# KMeans on profile features\n",
    "evaluate_clustering(KMeans, profile_df_pca, 'n_clusters', kmeans_params)\n",
    "\n",
    "# MeanShift on behavior features\n",
    "meanshift_bandwidths = [0.5, 1.0, 1.5, 2.0]\n",
    "evaluate_clustering(MeanShift, behavior_df_pca, 'bandwidth', meanshift_bandwidths)\n",
    "\n",
    "# MeanShift on profile features\n",
    "evaluate_clustering(MeanShift, profile_df_pca, 'bandwidth', meanshift_bandwidths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842225c3",
   "metadata": {},
   "source": [
    "# Next Steps: Merge Cluster Solutions\n",
    "\n",
    "After identifying the best clustering solutions for each perspective, merge them using hierarchical clustering on the centroids."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

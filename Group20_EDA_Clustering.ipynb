{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642b95d6",
   "metadata": {},
   "source": [
    "# Customer Clustering: Multi-Perspective Approach\n",
    "\n",
    "This notebook identifies customer clusters from different perspectives (behavioral, profile), compares clustering algorithms, and merges the best solutions using hierarchical clustering on centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans, MeanShift, estimate_bandwidth\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from umap import UMAP\n",
    "from minisom import MiniSom\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca93a",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "All custom functions for preprocessing, feature engineering, and aggregation are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_dataframe(df, columns, limits=(0.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Apply winsorization to each column in `columns`.\n",
    "    limits=(lower_pct, upper_pct) means: cap values at the 1st and 99th percentile.\n",
    "\n",
    "    Returns the winsorized copy of df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # winsorize returns masked arrays -> convert to normal array\n",
    "            df[col] = winsorize(df[col], limits=limits).data\n",
    "    return df\n",
    "\n",
    "def preprocess_flights(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the FlightsDB:\n",
    "    - Winsorize outliers\n",
    "    - Convert YearMonthDate to datetime\n",
    "    - Round down NumFlights and NumFlightsWithCompanions\n",
    "    - Set DistanceKM = 0 where NumFlights == 0\n",
    "    - Drop DollarCostPointsRedeemed\n",
    "    - Add log-transformed versions of skewed variables\n",
    "    - Create PointsUtilizationRatio = PointsRedeemed / PointsAccumulated\n",
    "    \"\"\"\n",
    "    df = flights_df.copy()\n",
    "\n",
    "    # 0. Winsorize outliers (Flights DB outliers are legitimate but skewed)\n",
    "    outlier_cols = [\n",
    "        'NumFlights', 'NumFlightsWithCompanions', 'DistanceKM', \n",
    "        'PointsAccumulated', 'PointsRedeemed'\n",
    "    ]\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 1. YearMonthDate -> datetime\n",
    "    if 'YearMonthDate' in df.columns:\n",
    "        df['YearMonthDate'] = pd.to_datetime(df['YearMonthDate'])\n",
    "\n",
    "    # 2. Round down flight counts and cast to int\n",
    "    for col in ['NumFlights', 'NumFlightsWithCompanions']:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.floor(df[col]).astype(int)\n",
    "\n",
    "    # 3. Fix logical inconsistency: DistanceKM must be 0 if NumFlights == 0\n",
    "    if {'NumFlights', 'DistanceKM'}.issubset(df.columns):\n",
    "        df.loc[df['NumFlights'] == 0, 'DistanceKM'] = 0\n",
    "\n",
    "    # 4. Drop perfectly correlated variable\n",
    "    if 'DollarCostPointsRedeemed' in df.columns:\n",
    "        df = df.drop(columns=['DollarCostPointsRedeemed'])\n",
    "\n",
    "    # 5. Log transforms for skewed numeric variables\n",
    "    log_cols = ['DistanceKM', 'PointsAccumulated', 'PointsRedeemed', 'NumFlights']\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # 6. Points utilisation ratio\n",
    "    if {'PointsRedeemed', 'PointsAccumulated'}.issubset(df.columns):\n",
    "        denom = df['PointsAccumulated'].replace({0: np.nan})\n",
    "        df['PointsUtilizationRatio'] = df['PointsRedeemed'] / denom\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_customers(customer_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the CustomerDB:\n",
    "    - Create cancellation flag from CancellationDate\n",
    "    - Group-median imputation (by LoyaltyStatus) for Income and Customer Lifetime Value\n",
    "    - Winsorize outliers (Income, CLV)\n",
    "    - Log transform Customer Lifetime Value and Income\n",
    "    - Encode Gender as binary\n",
    "    \"\"\"\n",
    "    df = customer_df.copy()\n",
    "\n",
    "    # 1. Cancellation flag\n",
    "    if 'CancellationDate' in df.columns:\n",
    "        df['CancelledFlag'] = df['CancellationDate'].notna().astype(int)\n",
    "\n",
    "    # 2. Group-median imputation by LoyaltyStatus\n",
    "    group_col = 'LoyaltyStatus'\n",
    "    cols_to_impute = ['Income', 'Customer Lifetime Value']\n",
    "    for col in cols_to_impute:\n",
    "        if col in df.columns and group_col in df.columns:\n",
    "            df[col] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "\n",
    "    # 3. Winsorize outliers\n",
    "    outlier_cols = ['Income', 'Customer Lifetime Value']\n",
    "    df = winsorize_dataframe(df, outlier_cols, limits=(0.01, 0.01))\n",
    "\n",
    "    # 4. Log transforms\n",
    "    if 'Customer Lifetime Value' in df.columns:\n",
    "        df['CLV_log'] = np.log1p(df['Customer Lifetime Value'])\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'].clip(lower=0))\n",
    "\n",
    "    # 5. Gender encoding\n",
    "    if 'Gender' in df.columns:\n",
    "        df['Gender'] = df['Gender'].map({'female': 1, 'male': 0}).fillna(0).astype(int)\n",
    "\n",
    "    # 6. Education to Years (Ordinal Encoding)\n",
    "    if 'Education' in df.columns:\n",
    "        edu_map = {\n",
    "            'High School or Below': 12,\n",
    "            'College': 14,\n",
    "            'Bachelor': 16,\n",
    "            'Master': 18,\n",
    "            'Doctor': 21\n",
    "        }\n",
    "        df['Education'] = df['Education'].map(edu_map)\n",
    "        df['Education'] = df['Education'].fillna(16)\n",
    "\n",
    "    # 7. Turn marital status into a flag\n",
    "    if 'Marital Status' in df.columns:\n",
    "        df['Marital Status'] = np.where(df['Marital Status'] != 'Married', 1, 0)\n",
    "\n",
    "    # 8. Tenure\n",
    "    ref_date = pd.to_datetime('2022-01-01')\n",
    "    if 'EnrollmentDateOpening' in df.columns:\n",
    "        df['EnrollmentDateOpening'] = pd.to_datetime(df['EnrollmentDateOpening'])\n",
    "        df['TenureMonths'] = (ref_date - df['EnrollmentDateOpening']) / pd.Timedelta(days=30.44)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_customer_flight_features(flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate monthly flight records into customer-level features:\n",
    "    - TotalFlights, TotalDistanceKM, TotalPointsAccumulated, TotalPointsRedeemed\n",
    "    - MeanPointsUtilization\n",
    "    - AverageFlightDistance\n",
    "    \"\"\"\n",
    "    id_col = 'Loyalty#'\n",
    "    df = flights_df.copy()\n",
    "    \n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(id_col)\n",
    "        .agg(\n",
    "            TotalFlights=('NumFlights', 'sum'),\n",
    "            TotalDistanceKM=('DistanceKM', 'sum'),\n",
    "            TotalPointsAccumulated=('PointsAccumulated', 'sum'),\n",
    "            TotalPointsRedeemed=('PointsRedeemed', 'sum'),\n",
    "            MeanPointsUtilization=('PointsUtilizationRatio', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Log transforms for aggregated features\n",
    "    for col in ['TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed']:\n",
    "        agg[f'{col}_log'] = np.log1p(agg[col])\n",
    "    \n",
    "    # Average flight distance\n",
    "    agg['AverageFlightDistance'] = agg['TotalDistanceKM'] / agg['TotalFlights'].replace({0: np.nan})\n",
    "\n",
    "    return agg\n",
    "\n",
    "def create_model_df(customer_df: pd.DataFrame, flights_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the final modeling dataframe:\n",
    "    1. Preprocess customers and flights\n",
    "    2. Build customer-level flight features\n",
    "    3. Merge datasets (Left Join)\n",
    "    4. Set Loyalty# as Index\n",
    "    5. Handle missing values\n",
    "    6. Encode categorical variables (OneHotEncoder)\n",
    "    7. Drop unnecessary columns\n",
    "    8. Scale numeric features (StandardScaler)\n",
    "    \"\"\"\n",
    "    # 1. Preprocess\n",
    "    cust_clean = preprocess_customers(customer_df)\n",
    "    flights_clean = preprocess_flights(flights_df)\n",
    "\n",
    "    # 2. Build flight features\n",
    "    flight_features = build_customer_flight_features(flights_clean)\n",
    "\n",
    "    # 3. Merge\n",
    "    model_df = cust_clean.merge(flight_features, on='Loyalty#', how='left')\n",
    "\n",
    "    # 4. Set Loyalty# as Index\n",
    "    if 'Loyalty#' in model_df.columns:\n",
    "        model_df.set_index('Loyalty#', inplace=True)\n",
    "\n",
    "    # 5. Handle Missing Values (Numeric)\n",
    "    numeric_cols_to_fill = model_df.select_dtypes(include=[np.number]).columns\n",
    "    model_df[numeric_cols_to_fill] = model_df[numeric_cols_to_fill].fillna(0)\n",
    "\n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'First Name', 'Last Name', 'CancellationDate', 'Customer Name',\n",
    "        'Country', 'Province or State', 'City', 'Postal Code',\n",
    "        'Latitude', 'Longitude', 'EnrollmentDateOpening', 'EnrollmentType',\n",
    "        'TotalFlights', 'TotalDistanceKM', 'TotalPointsAccumulated', 'TotalPointsRedeemed',\n",
    "        'Customer Lifetime Value', 'Income'\n",
    "    ]\n",
    "    model_df = model_df.drop(columns=[c for c in cols_to_drop if c in model_df.columns], errors='ignore')\n",
    "\n",
    "    # 7. Separate Numeric and Categorical\n",
    "    categorical_cols = ['LoyaltyStatus', 'Location Code']\n",
    "    categorical_cols = [c for c in categorical_cols if c in model_df.columns]\n",
    "       \n",
    "    numeric_cols = model_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude binary/ordinal from scaling\n",
    "    unscaled_cols = []\n",
    "    for col in ['CancelledFlag', 'Marital Status', 'Gender']:\n",
    "        if col in numeric_cols:\n",
    "            numeric_cols.remove(col)\n",
    "            unscaled_cols.append(col)\n",
    "\n",
    "    # 8. OneHotEncoding\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first', dtype=int)\n",
    "    encoded_data = ohe.fit_transform(model_df[categorical_cols])\n",
    "    encoded_cols = ohe.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=encoded_cols, index=model_df.index)\n",
    "    \n",
    "    # 9. Scale Numeric Features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_numeric = scaler.fit_transform(model_df[numeric_cols])\n",
    "    df_numeric_scaled = pd.DataFrame(scaled_numeric, columns=numeric_cols, index=model_df.index)\n",
    "    \n",
    "    # 10. Combine\n",
    "    dfs_to_concat = [df_numeric_scaled, df_encoded]\n",
    "    if unscaled_cols:\n",
    "        dfs_to_concat.append(model_df[unscaled_cols])\n",
    "        \n",
    "    df_final = pd.concat(dfs_to_concat, axis=1)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def evaluate_clustering(algorithm_cls, X, param_grid, verbose = True, **kwargs):\n",
    "    algo_name = algorithm_cls.__name__\n",
    "    best_labels = None\n",
    "    best_metrics = (-np.inf, np.inf, -np.inf) \n",
    "    \n",
    "    # Store original index to return later\n",
    "    original_index = getattr(X, 'index', None)\n",
    "    \n",
    "    # Convert to numpy for stable math calculations\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    for values in product(*[param_grid[k] for k in keys]):\n",
    "        params = dict(zip(keys, values))\n",
    "        model = algorithm_cls(**params, **kwargs)\n",
    "        labels = model.fit_predict(X_np)\n",
    "        \n",
    "        n_clusters = len(np.unique(labels))\n",
    "        if n_clusters <= 1:\n",
    "            continue\n",
    "\n",
    "        # Calculate Metrics\n",
    "        sil = round(silhouette_score(X_np, labels), 2)\n",
    "        db = round(davies_bouldin_score(X_np, labels), 2)\n",
    "        \n",
    "        # R2 calculation (Variance Explained)\n",
    "        # Using X_np ensures total_var is a single float, not a Series\n",
    "        overall_mean = X_np.mean(axis=0)\n",
    "        total_var = np.sum((X_np - overall_mean) ** 2)\n",
    "        \n",
    "        between_var = 0\n",
    "        for k in np.unique(labels):\n",
    "            cluster_data = X_np[labels == k]\n",
    "            cluster_mean = cluster_data.mean(axis=0)\n",
    "            n_k = len(cluster_data)\n",
    "            between_var += n_k * np.sum((cluster_mean - overall_mean) ** 2)\n",
    "            \n",
    "        r2 = round(between_var / total_var, 2) if total_var > 0 else 0.0\n",
    "\n",
    "        # Tie-breaking logic\n",
    "        current_metrics = (sil, db, r2)\n",
    "        is_better = False\n",
    "        if sil > best_metrics[0]:\n",
    "            is_better = True\n",
    "        elif sil == best_metrics[0]:\n",
    "            if db < best_metrics[1]:\n",
    "                is_better = True\n",
    "            elif db == best_metrics[1]:\n",
    "                if r2 > best_metrics[2]:\n",
    "                    is_better = True\n",
    "\n",
    "        if is_better:\n",
    "            best_metrics = current_metrics\n",
    "            best_labels = labels\n",
    "\n",
    "        param_str = ', '.join([f'{k}={v}' for k, v in params.items()])\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{algo_name} ({param_str}): Clusters={n_clusters}, Sil={sil}, DB={db}, R2={r2}\")\n",
    "\n",
    "    # Return as Series to preserve the index\n",
    "    if original_index is not None:\n",
    "        return pd.Series(best_labels, index=original_index, name=f\"{algo_name}_labels\")\n",
    "    return best_labels\n",
    "\n",
    "def plot_k_distance(df, title, ax=None):\n",
    "    # Rule of thumb: k = 2 * number of dimensions\n",
    "    k = 2 * len(df.columns)\n",
    "    \n",
    "    # Fit Nearest Neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    distances, _ = neigh.kneighbors(df)\n",
    "    \n",
    "    # Sort distances to the k-th neighbor\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "    \n",
    "    # Plotting logic\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    ax.plot(k_distances, color='steelblue', linewidth=2)\n",
    "    ax.set_title(f\"K-Distance: {title} (k={k})\", fontsize=12)\n",
    "    ax.set_xlabel(\"Points sorted by distance\", fontsize=10)\n",
    "    ax.set_ylabel(f\"Distance to {k}-th neighbor\", fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# UMAP helper: compute 2D embedding and plotting function\n",
    "def apply_umap_2d(X, n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42):\n",
    "    \"\"\"Return a pandas DataFrame with a 2D UMAP embedding for X.\"\"\"\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric=metric, random_state=random_state)\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    embedding = umap_model.fit_transform(X_np)\n",
    "    idx = X.index if isinstance(X, pd.DataFrame) else None\n",
    "    cols = ['UMAP1', 'UMAP2']\n",
    "    return pd.DataFrame(embedding, columns=cols, index=idx)\n",
    "\n",
    "def plot_umap(embedding, labels=None, palette='tab10', title='UMAP 2D', figsize=(8,6), alpha=0.8, s=30):\n",
    "    \"\"\"Plot 2D UMAP embedding. `embedding` can be a DataFrame or numpy array.\"\"\"\n",
    "    if isinstance(embedding, (np.ndarray, list)):\n",
    "        emb_df = pd.DataFrame(embedding, columns=['UMAP1','UMAP2'])\n",
    "    else:\n",
    "        emb_df = embedding.copy()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if labels is None:\n",
    "        sns.scatterplot(x=emb_df['UMAP1'], y=emb_df['UMAP2'], s=s, color='gray', alpha=alpha)\n",
    "    else:\n",
    "        # align labels to embedding index if possible\n",
    "        if isinstance(labels, pd.Series):\n",
    "            lab = labels.reindex(emb_df.index).values\n",
    "        else:\n",
    "            lab = labels\n",
    "        sns.scatterplot(x=emb_df['UMAP1'], y=emb_df['UMAP2'], hue=lab, palette=palette, s=s, alpha=alpha, legend='full')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.show()\n",
    "\n",
    "# Helper: train a MiniSom and return codebook DataFrame + BMU series\n",
    "def som_codebook_for_evaluation(X, x=10, y=10, sigma=1.0, learning_rate=0.5, n_iter=2000, random_seed=42):\n",
    "    \"\"\"Train a MiniSom on DataFrame X and return (som, codebook_df, bmu_series).\"\"\"\n",
    "    X_np = X.values\n",
    "    som = MiniSom(x, y, X_np.shape[1], sigma=sigma, learning_rate=learning_rate, random_seed=random_seed)\n",
    "    som.random_weights_init(X_np)\n",
    "    som.train_random(X_np, n_iter)\n",
    "    # get weights (x, y, features) and reshape to (x*y, features)\n",
    "    weights = som.get_weights()\n",
    "    codebook = weights.reshape(x * y, -1)\n",
    "    feature_names = X.columns.tolist()\n",
    "    codebook_df = pd.DataFrame(codebook, columns=feature_names)\n",
    "    # Map each sample to BMU node id (0..x*y-1)\n",
    "    bmus = [som.winner(xi) for xi in X_np]\n",
    "    node_ids = [i * y + j for (i, j) in bmus]\n",
    "    bmu_series = pd.Series(node_ids, index=X.index, name='SOM_node')\n",
    "    return som, codebook_df, bmu_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e363a",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "\n",
    "Load customer and flight data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ceb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "customer_db = pd.read_csv(\"data/DM_AIAI_CustomerDB.csv\", index_col=0 )\n",
    "flights_db = pd.read_csv(\"data/DM_AIAI_FlightsDB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfe1cb",
   "metadata": {},
   "source": [
    "# Feature Set Definition\n",
    "\n",
    "Define behavioral and profile feature sets for clustering perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257255fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in customer database\n",
    "initial_rows = customer_db.shape[0]\n",
    "duplicated_loyalty_ids = customer_db[customer_db['Loyalty#'].duplicated()]['Loyalty#'].unique()\n",
    "customer_db = customer_db.drop_duplicates(subset=['Loyalty#'])\n",
    "dropped_rows = initial_rows - customer_db.shape[0]\n",
    "dropped_percentage = (dropped_rows / initial_rows) * 100\n",
    "\n",
    "print(f\"Dropped {dropped_rows} duplicate customers ({dropped_percentage:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the modeling dataset using the pipeline function\n",
    "# This handles preprocessing, merging, missing values, encoding, feature selection AND scaling\n",
    "model_df = create_model_df(customer_db, flights_db)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7666a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories for profile and behavior segmentation\n",
    "behavior_features = [\n",
    "    'TotalFlights_log', 'TotalDistanceKM_log', 'TotalPointsAccumulated_log', \n",
    "    'TotalPointsRedeemed_log', 'MeanPointsUtilization', 'AverageFlightDistance', \n",
    "]\n",
    "\n",
    "profile_features = [\n",
    "    'CLV_log', 'Income_log', 'Gender', 'Education', 'Marital Status',\n",
    "    'LoyaltyStatus_Nova', 'LoyaltyStatus_Star', \n",
    "    'Location Code_Suburban', 'Location Code_Urban', 'TenureMonths',\n",
    "    'CancelledFlag'\n",
    "]\n",
    "\n",
    "print(f\"Behavior features: {behavior_features}\")\n",
    "print(f\"Profile features: {profile_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the model dataframe\n",
    "model_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208e24c",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "Analyze and remove highly correlated features to improve clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4707b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Feature Selection\n",
    "# 1. Plot initial correlation matrix\n",
    "plt.figure(figsize=(24, 20))\n",
    "initial_corr = model_df.corr()\n",
    "sns.heatmap(initial_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Initial Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 2. Identify and remove highly correlated features (> 0.8)\n",
    "# We use the absolute correlation matrix\n",
    "corr_matrix = model_df.corr().abs()\n",
    "\n",
    "# We select the upper triangle of the correlation matrix. \n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# We drop the column (Feature B) if it has a correlation > 0.8 with any previous column (Feature A).\n",
    "# This way, Feature A is kept and Feature B is removed.\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "print(f\"Dropping highly correlated features: {to_drop}\")\n",
    "\n",
    "# Drop features from model_df\n",
    "model_df = model_df.drop(columns=to_drop)\n",
    "\n",
    "# Update feature lists to remove dropped columns\n",
    "behavior_features = [f for f in behavior_features if f not in to_drop]\n",
    "profile_features = [f for f in profile_features if f not in to_drop]\n",
    "\n",
    "print(f\"Updated behavior features: {behavior_features}\")\n",
    "print(f\"Updated profile features: {profile_features}\")\n",
    "\n",
    "# 3. Plot updated correlation matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(model_df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing Highly Correlated Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7e97d",
   "metadata": {},
   "source": [
    "# Outlier Detection\n",
    "\n",
    "Detect and remove multivariate outliers using DBSCAN before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4061e8",
   "metadata": {},
   "source": [
    "DBSCAN is applied to the scaled feature space for outlier detection. In this project, DBSCAN is used exclusively for identifying anomalous customers, not for final segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3419c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model_df as X_scaled (Loyalty# is now the index)\n",
    "dbscan = DBSCAN(eps=1.9, min_samples=20, n_jobs=1)\n",
    "dbscan_labels = dbscan.fit_predict(model_df)\n",
    "\n",
    "outlier_count = Counter(dbscan_labels)\n",
    "print(f\"DBSCAN results: {outlier_count}\")\n",
    "print(f\"Outliers detected: {outlier_count.get(-1, 0)}\")\n",
    "print(f\"Core customers: {outlier_count.get(0, 0)}\")\n",
    "\n",
    "core_mask = (dbscan_labels != -1)\n",
    "\n",
    "model_df_clipped = model_df[core_mask]\n",
    "outliers_df = model_df[dbscan_labels == -1]\n",
    "\n",
    "print(f\"Core customers kept: {len(model_df_clipped):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b6a4a",
   "metadata": {},
   "source": [
    "# Clustering Algorithms Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eb7fd",
   "metadata": {},
   "source": [
    "## Clustering: Multiple Algorithms & Perspectives\n",
    "\n",
    "We now test several clustering algorithms (KMeans, DBSCAN, MeanShift) with different hyperparameters, This approach allows us to compare cluster quality and interpretability across perspectives and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature sets for clustering\n",
    "behavior_df = model_df_clipped[behavior_features]\n",
    "profile_df = model_df_clipped[profile_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fcca1",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on behavior features\n",
    "kmeans_param_grid = {'n_clusters': [2, 3, 4, 5, 6], 'init':['k-means++'], 'random_state':[42]}\n",
    "kmeans_behavior_labels = evaluate_clustering(KMeans, behavior_df, kmeans_param_grid)\n",
    "kmeans_behavior_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_profile_labels = evaluate_clustering(KMeans, profile_df, kmeans_param_grid)\n",
    "kmeans_profile_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba321d59",
   "metadata": {},
   "source": [
    "## Meanshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8553780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift on behavior features\n",
    "bandwidth = estimate_bandwidth(behavior_df, quantile=0.2, n_samples=500)\n",
    "meanshift_param_grid = {'bandwidth': [bandwidth], 'n_jobs': [-1]}\n",
    "meanshift_behavior_labels = evaluate_clustering(MeanShift, behavior_df, meanshift_param_grid)\n",
    "meanshift_behavior_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift on profile features\n",
    "bandwidth = estimate_bandwidth(profile_df, quantile=0.2, n_samples=500)\n",
    "meanshift_param_grid = {'bandwidth': [bandwidth], 'n_jobs': [-1]}\n",
    "meanshift_profile_labels = evaluate_clustering(MeanShift, profile_df, meanshift_param_grid)\n",
    "meanshift_profile_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda644b8",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6979024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate figures\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Behavior DataFrame\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot_k_distance(behavior_df, \"Behavior Data\", ax=ax1)\n",
    "\n",
    "# Subplot 2: Profile DataFrame\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot_k_distance(profile_df, \"Profile Data\", ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on profile features\n",
    "dbscan_param_grid = {'eps': [0.2, 0.3, 0.4], 'min_samples': [10, 15, 20], 'n_jobs': [-1]}\n",
    "dbscan_behavior_labels = evaluate_clustering(DBSCAN, behavior_df, dbscan_param_grid)\n",
    "dbscan_behavior_labels.value_counts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90661d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on profile features\n",
    "dbscan_param_grid = {'eps': [1.4, 1.5, 1.6], 'min_samples': [10, 15, 20], 'n_jobs': [-1]}\n",
    "dbscan_profile_labels = evaluate_clustering(DBSCAN, profile_df, dbscan_param_grid)\n",
    "dbscan_profile_labels.value_counts()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f55db1",
   "metadata": {},
   "source": [
    "# Self Organizing Maps\n",
    "\n",
    "Simple workflow: train a SOM per perspective, cluster the SOM codebook with KMeans (use `evaluate_clustering`),\n",
    "and map neuron clusters back to samples. For each perspective we also plot the U‑matrix and hitmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM on behavior_df (using som_codebook_for_evaluation)\n",
    "som_behavior, codebook_beh, bmu_series_beh = som_codebook_for_evaluation(behavior_df)\n",
    "\n",
    "neuron_labels_beh = evaluate_clustering(KMeans, codebook_beh, verbose = False, param_grid=kmeans_param_grid)\n",
    "\n",
    "som_labels_beh = bmu_series_beh.map(neuron_labels_beh)\n",
    "\n",
    "# Calculate silhouette score for SOM clusters on behavior data\n",
    "som_silhouette_beh = silhouette_score(behavior_df, som_labels_beh)\n",
    "print(f\"SOM Clustering on Behavior Data: Silhouette Score = {som_silhouette_beh:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM on behavior_df (using som_codebook_for_evaluation)\n",
    "som_behavior, codebook_pro, bmu_series_pro = som_codebook_for_evaluation(profile_df)\n",
    "\n",
    "neuron_labels_pro = evaluate_clustering(KMeans, codebook_pro, verbose=False, param_grid=kmeans_param_grid)\n",
    "\n",
    "som_labels_pro = bmu_series_pro.map(neuron_labels_pro)\n",
    "\n",
    "# Calculate silhouette score for SOM clusters on profile data\n",
    "som_silhouette_pro = silhouette_score(profile_df, som_labels_pro)\n",
    "print(f\"SOM Clustering on Profile Data: Silhouette Score = {som_silhouette_pro:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842225c3",
   "metadata": {},
   "source": [
    "# Next Steps: Merge Cluster Solutions\n",
    "\n",
    "After identifying the best clustering solutions for each perspective, merge them using hierarchical clustering on the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e97bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_centroids = list(behavior_df.columns) + [c for c in profile_df.columns if c not in behavior_df.columns]\n",
    "\n",
    "df = model_df_clipped[features_for_centroids].copy()\n",
    "\n",
    "df[\"behavior_cluster\"] = kmeans_behavior_labels.reindex(df.index).astype(int)\n",
    "df[\"profile_cluster\"]  = kmeans_profile_labels.reindex(df.index).astype(int)\n",
    "\n",
    "df_centroids = df.groupby([\"behavior_cluster\", \"profile_cluster\"])[features_for_centroids].mean()\n",
    "\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== HC on centroids (com tracking de distâncias) =====\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "centroid_feature_cols = [c for c in df_centroids.columns if c != \"segment_size\"]\n",
    "\n",
    "X_centroids = df_centroids[centroid_feature_cols].values  # matriz (n_centroids x n_features)\n",
    "\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=\"ward\",\n",
    "    metric=\"euclidean\",\n",
    "    distance_threshold=0,\n",
    "    n_clusters=None\n",
    ")\n",
    "\n",
    "hclust_labels = hclust.fit_predict(X_centroids)\n",
    "\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            current_count += 1  # leaf\n",
    "        else:\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "linkage_matrix = np.column_stack([\n",
    "    hclust.children_,\n",
    "    hclust.distances_,\n",
    "    counts\n",
    "]).astype(float)\n",
    "\n",
    "print(\"Linkage matrix ready for dendrogram!\")\n",
    "print(\"n_centroids:\", X_centroids.shape[0], \"| n_features:\", X_centroids.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9424867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Plot dendrogram=====\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "y_threshold = 2.3\n",
    "\n",
    "labels = [\n",
    "    f\"{idx[0]}-{idx[1]} (n={int(df_centroids.loc[idx, 'segment_size'])})\"\n",
    "    if \"segment_size\" in df_centroids.columns\n",
    "    else f\"{idx[0]}-{idx[1]}\"\n",
    "    for idx in df_centroids.index\n",
    "]\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=labels,\n",
    "    color_threshold=y_threshold,\n",
    "    above_threshold_color=\"k\"\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    y=y_threshold,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Cut at {y_threshold}\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Hierarchical Clustering on {len(df_centroids)} Centroids\\n(Ward Linkage)\",\n",
    "    fontsize=16\n",
    ")\n",
    "plt.xlabel(\"Combination (behavior_cluster, profile_cluster)\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
